{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2244c89a-965f-49bd-8056-194018c9f8b1",
   "metadata": {},
   "source": [
    "### S3로 데이터 적재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9fb12e8-3837-40bf-b5a8-2414443fe5a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-04T06:08:01.136430Z",
     "iopub.status.busy": "2022-10-04T06:08:01.135984Z",
     "iopub.status.idle": "2022-10-04T06:08:01.141841Z",
     "shell.execute_reply": "2022-10-04T06:08:01.141397Z",
     "shell.execute_reply.started": "2022-10-04T06:08:01.136355Z"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT = \"emart-datafabric\"\n",
    "DATASET = \"common_dev\"\n",
    "TABLE = \"dfm_sample_eapp_review_keywords\"\n",
    "LIMIT = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd0fc0e5-46f4-441a-a11b-6d2988199647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-04T06:08:03.880397Z",
     "iopub.status.busy": "2022-10-04T06:08:03.880020Z",
     "iopub.status.idle": "2022-10-04T06:08:03.965995Z",
     "shell.execute_reply": "2022-10-04T06:08:03.965494Z",
     "shell.execute_reply.started": "2022-10-04T06:08:03.880373Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydatafabric.vault_utils import get_secrets\n",
    "\n",
    "aws_info = get_secrets(mount_point=\"datafabric\",path=\"aws/credentials/datafabric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0939c5b6-f6f7-4a11-8690-77a6721db283",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-04T06:08:05.058155Z",
     "iopub.status.busy": "2022-10-04T06:08:05.057784Z",
     "iopub.status.idle": "2022-10-04T06:08:13.086183Z",
     "shell.execute_reply": "2022-10-04T06:08:13.085511Z",
     "shell.execute_reply.started": "2022-10-04T06:08:05.058133Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/04 15:08:08 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "22/10/04 15:08:08 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "22/10/04 15:08:08 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/10/04 15:08:08 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "from pydatafabric.ye import get_spark\n",
    "import os\n",
    "\n",
    "spark = get_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3cfcdd-cbc3-4806-8d42-2d9be4498845",
   "metadata": {},
   "source": [
    "### Spark 설정 필수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4be712a0-85c3-4f13-af26-45973acbeb98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T04:02:28.913857Z",
     "iopub.status.busy": "2022-09-30T04:02:28.913316Z",
     "iopub.status.idle": "2022-09-30T04:02:28.920400Z",
     "shell.execute_reply": "2022-09-30T04:02:28.919940Z",
     "shell.execute_reply.started": "2022-09-30T04:02:28.913827Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.s3a.fast.upload.buffer\", \"disk\")\n",
    "spark.conf.set(\"fs.s3a.buffer.dir\", \"/tmp\")\n",
    "spark.conf.set(\"fs.s3a.access.key\", aws_info[\"aws_access_key_id\"])\n",
    "spark.conf.set(\"fs.s3a.secret.key\", aws_info[\"aws_secret_access_key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95e68d80-c835-44db-a5d5-685215c13bd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-04T06:08:38.418145Z",
     "iopub.status.busy": "2022-10-04T06:08:38.417769Z",
     "iopub.status.idle": "2022-10-04T06:08:41.195643Z",
     "shell.execute_reply": "2022-10-04T06:08:41.195038Z",
     "shell.execute_reply.started": "2022-10-04T06:08:38.418120Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "review_keywords = spark.read.format('bigquery') \\\n",
    "  .option('table', f\"{PROJECT}:{DATASET}.{TABLE}\") \\\n",
    "  .load()\n",
    "review_keywords.createOrReplaceTempView('temp_review_keywords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c5354b-b993-467f-a37c-080c4514e7b7",
   "metadata": {},
   "source": [
    "### Temp View에서 질의를 통해 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "644152aa-71d6-49e5-9521-1e2599619bab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-04T06:08:42.491246Z",
     "iopub.status.busy": "2022-10-04T06:08:42.490882Z",
     "iopub.status.idle": "2022-10-04T06:08:42.494090Z",
     "shell.execute_reply": "2022-10-04T06:08:42.493622Z",
     "shell.execute_reply.started": "2022-10-04T06:08:42.491210Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query=\"select * from temp_review_keywords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55424209-ddd0-4647-ae61-f479eba261b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-04T06:08:43.783248Z",
     "iopub.status.busy": "2022-10-04T06:08:43.782845Z",
     "iopub.status.idle": "2022-10-04T06:08:49.097178Z",
     "shell.execute_reply": "2022-10-04T06:08:49.096509Z",
     "shell.execute_reply.started": "2022-10-04T06:08:43.783211Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+------------------------------------+------------------------+----------------------------------+----------+-------------------------------+\n",
      "|      prdt_cd|store_cd|order_date|                            sku_name|           link_sku_name|                           comment|score_text|                       keywords|\n",
      "+-------------+--------+----------+------------------------------------+------------------------+----------------------------------+----------+-------------------------------+\n",
      "|1102150000000|    1018|  20220102|(후레쉬팩)미국냉장초이스척아이로스트|(후레쉬팩)미국산CH갈비살|    기름기가 많아서 \n",
      "먹기 불편했음|       Bad|    기름기 많아서 먹기 불편했음|\n",
      "|1102150000000|    1108|  20220101|(후레쉬팩)미국냉장초이스척아이로스트|(후레쉬팩)미국산CH갈비살| 기름이 너무너무 많아요.버린 기...|       Bad|       기름 많아요 기름 무게 만|\n",
      "|1102150000000|    1108|  20220101|(후레쉬팩)미국냉장초이스척아이로스트|(후레쉬팩)미국산CH갈비살|세일전주에 샀을때 손질해보니 기...|       Bad|전주 때 손질 기름 근육 고기 ...|\n",
      "|1102150000000|    1048|  20220101|(후레쉬팩)미국냉장초이스척아이로스트|(후레쉬팩)미국산CH갈비살| 고기가 상태가 별로였어요\n",
      "빨리 ...|       Bad|    고기 상태 별로 상한 고기 변|\n",
      "|1102150000000|    1150|  20220101|(후레쉬팩)미국냉장초이스척아이로스트|(후레쉬팩)미국산CH갈비살|   오~~\n",
      "요건 거격이 저렴해도 앞...|       Bad|오 요건 거격 저렴해도 앞 안 ...|\n",
      "+-------------+--------+----------+------------------------------------+------------------------+----------------------------------+----------+-------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.sql(query)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee5e9c-8014-4667-b060-99ab5729720d",
   "metadata": {},
   "source": [
    "### S3로 데이터 적재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceb4efa2-346c-4133-a48e-b6728712bf56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-04T06:08:55.355174Z",
     "iopub.status.busy": "2022-10-04T06:08:55.354797Z",
     "iopub.status.idle": "2022-10-04T06:09:48.788908Z",
     "shell.execute_reply": "2022-10-04T06:09:48.788078Z",
     "shell.execute_reply.started": "2022-10-04T06:08:55.355152Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o118.parquet.\n: java.net.SocketTimeoutException: doesBucketExist on emart-datafabric-dev: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: Unable to load credentials from service endpoint\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateInterruptedException(S3AUtils.java:342)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:177)\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:111)\n\tat org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:391)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:322)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3376)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:126)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3425)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3393)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:486)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:470)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:572)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: Unable to load credentials from service endpoint\n\tat org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:159)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1225)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:801)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:751)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4920)\n\tat com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:5700)\n\tat com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:5673)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4904)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4866)\n\tat com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1394)\n\tat com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1333)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:392)\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)\n\t... 29 more\nCaused by: com.amazonaws.SdkClientException: Unable to load credentials from service endpoint\n\tat com.amazonaws.auth.EC2CredentialsFetcher.handleError(EC2CredentialsFetcher.java:183)\n\tat com.amazonaws.auth.EC2CredentialsFetcher.fetchCredentials(EC2CredentialsFetcher.java:162)\n\tat com.amazonaws.auth.EC2CredentialsFetcher.getCredentials(EC2CredentialsFetcher.java:82)\n\tat com.amazonaws.auth.InstanceProfileCredentialsProvider.getCredentials(InstanceProfileCredentialsProvider.java:172)\n\tat org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:137)\n\t... 47 more\nCaused by: java.net.SocketTimeoutException: connect timed out\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:607)\n\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:463)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:558)\n\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:242)\n\tat sun.net.www.http.HttpClient.New(HttpClient.java:339)\n\tat sun.net.www.http.HttpClient.New(HttpClient.java:357)\n\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1223)\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1202)\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)\n\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990)\n\tat com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:54)\n\tat com.amazonaws.internal.EC2CredentialsUtils.readResource(EC2CredentialsUtils.java:113)\n\tat com.amazonaws.internal.EC2CredentialsUtils.readResource(EC2CredentialsUtils.java:82)\n\tat com.amazonaws.auth.InstanceProfileCredentialsProvider$InstanceMetadataCredentialsEndpointProvider.getCredentialsEndpoint(InstanceProfileCredentialsProvider.java:197)\n\tat com.amazonaws.auth.EC2CredentialsFetcher.fetchCredentials(EC2CredentialsFetcher.java:122)\n\t... 50 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m bucket \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memart-datafabric-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m s3_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3a://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucket\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/bigquery-db\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(query)\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(s3_path)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:1250\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1250\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o118.parquet.\n: java.net.SocketTimeoutException: doesBucketExist on emart-datafabric-dev: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: Unable to load credentials from service endpoint\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateInterruptedException(S3AUtils.java:342)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:177)\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:111)\n\tat org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:391)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:322)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3376)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:126)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3425)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3393)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:486)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:470)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:572)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: Unable to load credentials from service endpoint\n\tat org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:159)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1225)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:801)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:751)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4920)\n\tat com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:5700)\n\tat com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:5673)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4904)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4866)\n\tat com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1394)\n\tat com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1333)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:392)\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)\n\t... 29 more\nCaused by: com.amazonaws.SdkClientException: Unable to load credentials from service endpoint\n\tat com.amazonaws.auth.EC2CredentialsFetcher.handleError(EC2CredentialsFetcher.java:183)\n\tat com.amazonaws.auth.EC2CredentialsFetcher.fetchCredentials(EC2CredentialsFetcher.java:162)\n\tat com.amazonaws.auth.EC2CredentialsFetcher.getCredentials(EC2CredentialsFetcher.java:82)\n\tat com.amazonaws.auth.InstanceProfileCredentialsProvider.getCredentials(InstanceProfileCredentialsProvider.java:172)\n\tat org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:137)\n\t... 47 more\nCaused by: java.net.SocketTimeoutException: connect timed out\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:607)\n\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:463)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:558)\n\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:242)\n\tat sun.net.www.http.HttpClient.New(HttpClient.java:339)\n\tat sun.net.www.http.HttpClient.New(HttpClient.java:357)\n\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1223)\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1202)\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)\n\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990)\n\tat com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:54)\n\tat com.amazonaws.internal.EC2CredentialsUtils.readResource(EC2CredentialsUtils.java:113)\n\tat com.amazonaws.internal.EC2CredentialsUtils.readResource(EC2CredentialsUtils.java:82)\n\tat com.amazonaws.auth.InstanceProfileCredentialsProvider$InstanceMetadataCredentialsEndpointProvider.getCredentialsEndpoint(InstanceProfileCredentialsProvider.java:197)\n\tat com.amazonaws.auth.EC2CredentialsFetcher.fetchCredentials(EC2CredentialsFetcher.java:122)\n\t... 50 more\n"
     ]
    }
   ],
   "source": [
    "env = \"dev\" \n",
    "bucket = f\"emart-datafabric-{env}\"\n",
    "\n",
    "\n",
    "s3_path = f\"s3a://{bucket}/bigquery-db\"\n",
    "spark.sql(query).write.mode(\"overwrite\").parquet(s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0d779ed-1c12-46b1-9ed6-48683c446ccd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T04:03:53.083116Z",
     "iopub.status.busy": "2022-09-30T04:03:53.082704Z",
     "iopub.status.idle": "2022-09-30T04:03:56.350653Z",
     "shell.execute_reply": "2022-09-30T04:03:56.349926Z",
     "shell.execute_reply.started": "2022-09-30T04:03:53.083086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-30 13:03:54,208 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2022-09-30 13:03:54,259 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2022-09-30 13:03:54,259 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "Found 2 items\n",
      "-rw-rw-rw-   1 datafabric datafabric          0 2022-09-30 13:03 s3a://emart-datafabric-dev/bigquery-db/_SUCCESS\n",
      "-rw-rw-rw-   1 datafabric datafabric   14702163 2022-09-30 13:03 s3a://emart-datafabric-dev/bigquery-db/part-00000-12819580-5b71-4929-8823-63f2c574f7b5-c000.snappy.parquet\n",
      "2022-09-30 13:03:55,901 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2022-09-30 13:03:55,901 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2022-09-30 13:03:55,901 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -Dhadoop.security.credential.provider.path=jceks:///datafabric/credentials/aws.jceks -ls {s3_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4701b9f6-d005-4f87-b34c-b61182cd8d7a",
   "metadata": {},
   "source": [
    "### Spark Context 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4dc5439-8f16-4be0-b910-7ba09367e1aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T04:03:59.981008Z",
     "iopub.status.busy": "2022-09-30T04:03:59.980588Z",
     "iopub.status.idle": "2022-09-30T04:04:00.774477Z",
     "shell.execute_reply": "2022-09-30T04:04:00.773869Z",
     "shell.execute_reply.started": "2022-09-30T04:03:59.980980Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d380a876-b1cd-4026-ac16-2060c4c9a50c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
