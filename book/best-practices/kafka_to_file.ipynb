{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b132e6-352b-49c1-b8a3-f286408f923b",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8f30f8-ebb6-48ac-887e-9ce17467cb85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T03:45:13.031983Z",
     "iopub.status.busy": "2022-09-28T03:45:13.031522Z",
     "iopub.status.idle": "2022-09-28T03:45:13.038063Z",
     "shell.execute_reply": "2022-09-28T03:45:13.037541Z",
     "shell.execute_reply.started": "2022-09-28T03:45:13.031895Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "BOOTSTRAP_SERVERS = 'b-2.emartai-streamhub-d.bnlyhx.c2.kafka.ap-northeast-2.amazonaws.com:9094,b-1.emartai-streamhub-d.bnlyhx.c2.kafka.ap-northeast-2.amazonaws.com:9094,b-3.emartai-streamhub-d.bnlyhx.c2.kafka.ap-northeast-2.amazonaws.com:9094'\n",
    "CONSUMER_CONFIGS = 'security.protocol=SSL,ssl.truststore.location=kafka-to-bigquery_certs_emartai-streamhub-datalake-cluster-stg_kafka.client.truststore.jks,ssl.truststore.password=changeit,ssl.keystore.location=kafka-to-bigquery_certs_emartai-streamhub-datalake-cluster-stg_kafka.client.keystore.jks,ssl.keystore.password=changeit,ssl.key.password=changeit'\n",
    "INPUT_TOPICS = 'tfactory-checkin'\n",
    "CONSUMER_GROUP = 'test-group-1'\n",
    "OUTPUT_TABLE = 'test_db.test_table'\n",
    "OUTPUT_VIEW = 'test_db.test_view'\n",
    "OUTPUT_PATH = 'hdfs:///tmp/test/test_table/'\n",
    "DT = '20210605'\n",
    "JOB = 'kafka-to-ye-hive-tfactory-checkin-test_db.test_table-20210605'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eaeaa6-f961-41f2-a2b8-934080edde4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T03:45:13.792602Z",
     "iopub.status.busy": "2022-09-28T03:45:13.792271Z",
     "iopub.status.idle": "2022-09-28T03:45:13.795426Z",
     "shell.execute_reply": "2022-09-28T03:45:13.794932Z",
     "shell.execute_reply.started": "2022-09-28T03:45:13.792581Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HADOOP_USER_NAME'] = 'airflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c4a751-d53e-4760-b790-4a24dff27c4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T03:45:15.176058Z",
     "iopub.status.busy": "2022-09-28T03:45:15.175647Z",
     "iopub.status.idle": "2022-09-28T03:45:15.182629Z",
     "shell.execute_reply": "2022-09-28T03:45:15.182105Z",
     "shell.execute_reply.started": "2022-09-28T03:45:15.176035Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['security.protocol', 'SSL']\n",
      "['ssl.truststore.location', 'kafka-to-bigquery_certs_emartai-streamhub-datalake-cluster-stg_kafka.client.truststore.jks']\n",
      "['ssl.truststore.password', 'changeit']\n",
      "['ssl.keystore.location', 'kafka-to-bigquery_certs_emartai-streamhub-datalake-cluster-stg_kafka.client.keystore.jks']\n",
      "['ssl.keystore.password', 'changeit']\n",
      "['ssl.key.password', 'changeit']\n",
      "\n",
      "env: REMOTE_FILES=\n",
      "\n"
     ]
    }
   ],
   "source": [
    "consumer_config_list = []\n",
    "remote_file_list = []\n",
    "spark_file_list = []\n",
    "\n",
    "for config in CONSUMER_CONFIGS.split(','):\n",
    "    tokens = config.split('=')\n",
    "    key = tokens[0]\n",
    "    value = tokens[1]\n",
    "    if value.startswith('hdfs://'):\n",
    "        remote_file_list.append(value)\n",
    "        \n",
    "        filename = value.split('/').pop()\n",
    "        spark_file_list.append(filename)\n",
    "        \n",
    "        consumer_config_list.append(f'{key}={filename}')\n",
    "    else:\n",
    "        consumer_config_list.append(f'{key}={value}')\n",
    "        \n",
    "CONSUMER_CONFIGS = ','.join(consumer_config_list)\n",
    "print(CONSUMER_CONFIGS)\n",
    "\n",
    "REMOTE_FILES = ','.join(remote_file_list)\n",
    "print(REMOTE_FILES)\n",
    "%env REMOTE_FILES={REMOTE_FILES}\n",
    "\n",
    "SPARK_FILES = ','.join(spark_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2e3e437-b5fb-47ca-8284-99acd9a4541e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T03:45:34.482667Z",
     "iopub.status.busy": "2022-09-28T03:45:34.482291Z",
     "iopub.status.idle": "2022-09-28T03:45:47.459858Z",
     "shell.execute_reply": "2022-09-28T03:45:47.459211Z",
     "shell.execute_reply.started": "2022-09-28T03:45:34.482644Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ set -ex\n",
      "+ str=\n",
      "+ readarray -d , -t strarr\n",
      "+ (( n=0 ))\n",
      "+ (( n<1 ))\n",
      "+ echo\n",
      "+ hadoop fs -copyToLocal -f .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ (( n++  ))\n",
      "+ (( n<1 ))\n"
     ]
    }
   ],
   "source": [
    "%%bash -x\n",
    "\n",
    "set -ex\n",
    "\n",
    "str=$REMOTE_FILES\n",
    "\n",
    "readarray -d , -t strarr <<<\"$str\"\n",
    "for (( n=0; n<${#strarr[*]}; n++ ))  \n",
    "do\n",
    "    echo ${strarr[n]}\n",
    "    hadoop fs -copyToLocal -f ${strarr[n]} .\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b58ee222-bc05-466a-8783-534857147373",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T03:51:34.138047Z",
     "iopub.status.busy": "2022-09-28T03:51:34.137670Z",
     "iopub.status.idle": "2022-09-28T03:51:36.982060Z",
     "shell.execute_reply": "2022-09-28T03:51:36.981222Z",
     "shell.execute_reply.started": "2022-09-28T03:51:34.138019Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"main\" java.io.IOException: Error accessing gs://external_libs/spark/jars/spark-bigquery-with-dependencies_2.11-0.16.1.jar\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2140)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2032)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n",
      "\tat org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:65)\n",
      "\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:281)\n",
      "\tat org.apache.hadoop.fs.Globber.glob(Globber.java:149)\n",
      "\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2069)\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1098)\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1059)\n",
      "\tat org.apache.spark.deploy.DependencyUtils$.resolveGlobPath(DependencyUtils.scala:192)\n",
      "\tat org.apache.spark.deploy.DependencyUtils$.$anonfun$resolveGlobPaths$2(DependencyUtils.scala:147)\n",
      "\tat org.apache.spark.deploy.DependencyUtils$.$anonfun$resolveGlobPaths$2$adapted(DependencyUtils.scala:145)\n",
      "\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n",
      "\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n",
      "\tat org.apache.spark.deploy.DependencyUtils$.resolveGlobPaths(DependencyUtils.scala:145)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$4(SparkSubmit.scala:363)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:363)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n",
      "GET https://storage.googleapis.com/storage/v1/b/external_libs/o/spark%2Fjars%2Fspark-bigquery-with-dependencies_2.11-0.16.1.jar?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n",
      "{\n",
      "  \"code\" : 403,\n",
      "  \"errors\" : [ {\n",
      "    \"domain\" : \"global\",\n",
      "    \"message\" : \"Request is prohibited by organization's policy. vpcServiceControlsUniqueIdentifier: HFD3p2DnzxELxq07xfhqJ2qKbWboldQWe_3SGFQ3ZAYBV3GwxsotiA\",\n",
      "    \"reason\" : \"vpcServiceControls\"\n",
      "  } ],\n",
      "  \"message\" : \"Request is prohibited by organization's policy. vpcServiceControlsUniqueIdentifier: HFD3p2DnzxELxq07xfhqJ2qKbWboldQWe_3SGFQ3ZAYBV3GwxsotiA\"\n",
      "}\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2134)\n",
      "\t... 31 more\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m     spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.execution.arrow.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spark\n\u001b[0;32m---> 72\u001b[0m spark \u001b[38;5;241m=\u001b[39m get_spark_for_kafka()\n",
      "Cell \u001b[0;32mIn [8], line 48\u001b[0m, in \u001b[0;36mget_spark_for_kafka\u001b[0;34m(scale, queue)\u001b[0m\n\u001b[1;32m     26\u001b[0m     spark \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     27\u001b[0m         SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.app.name\u001b[39m\u001b[38;5;124m\"\u001b[39m, app_name)\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.driver.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscale\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m8\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     45\u001b[0m     )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     spark \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 48\u001b[0m         \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.app.name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapp_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m6g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m8g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.shuffle.service.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.dynamicAllocation.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.dynamicAllocation.maxExecutors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m200\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.maxResultSize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m6g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.rpc.message.maxSize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1024\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.yarn.queue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.ui.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfalse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.port.maxRetries\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m128\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.yarn.appMasterEnv.ARROW_PRE_0_15_IPC_FORMAT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.jars\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgs://external_libs/spark/jars/spark-bigquery-with-dependencies_2.11-0.16.1.jar,gs://external_libs/spark/jars/spark-sql-kafka/net.jpountz.lz4_lz4-1.3.0.jar,gs://external_libs/spark/jars/spark-sql-kafka/org.apache.kafka_kafka-clients-0.10.0.1.jar,gs://external_libs/spark/jars/spark-sql-kafka/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.1.jar,gs://external_libs/spark/jars/spark-sql-kafka/org.slf4j_slf4j-api-1.7.16.jar,gs://external_libs/spark/jars/spark-sql-kafka/org.spark-project.spark_unused-1.0.0.jar,gs://external_libs/spark/jars/spark-sql-kafka/org.xerial.snappy_snappy-java-1.1.2.6.jar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.files\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSPARK_FILES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menableHiveSupport\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     69\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.execution.arrow.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m spark\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py:384\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 144\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py:331\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 331\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/java_gateway.py:108\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    111\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "from pydatafabric.vault_utils import get_secrets\n",
    "\n",
    "def get_spark_for_kafka(scale=0, queue=None):\n",
    "    import os\n",
    "    import uuid\n",
    "    import tempfile\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pydatafabric.vault_utils import get_secrets\n",
    "\n",
    "    tmp_uuid = str(uuid.uuid4())\n",
    "    app_name = f\"emart-{os.environ.get('USER', 'default')}-{tmp_uuid}\"\n",
    "    if not queue:\n",
    "        if \"JUPYTERHUB_USER\" in os.environ:\n",
    "            queue = \"dmig_eda\"\n",
    "        else:\n",
    "            queue = \"airflow_job\"\n",
    "    os.environ[\"ARROW_PRE_0_15_IPC_FORMAT\"] = \"1\"\n",
    "\n",
    "    key = get_secrets(mount_point=\"datafabric\",path=\"gcp/emart-datafabric/datafabric\")[\"config\"]    \n",
    "    key_file_name = tempfile.mkstemp()[1]\n",
    "    with open(key_file_name, \"wb\") as key_file:\n",
    "        key_file.write(key.encode())\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = key_file.name\n",
    "\n",
    "    if scale in [1, 2, 3, 4]:\n",
    "        spark = (\n",
    "            SparkSession.builder.config(\"spark.app.name\", app_name)\n",
    "            .config(\"spark.driver.memory\", f\"{scale*8}g\")\n",
    "            .config(\"spark.executor.memory\", f\"{scale*3}g\")\n",
    "            .config(\"spark.executor.instances\", f\"{scale*8}\")\n",
    "            .config(\"spark.driver.maxResultSize\", f\"{scale*4}g\")\n",
    "            .config(\"spark.rpc.message.maxSize\", \"1024\")\n",
    "            .config(\"spark.yarn.queue\", queue)\n",
    "            .config(\"spark.ui.enabled\", \"false\")\n",
    "            .config(\"spark.port.maxRetries\", \"128\")\n",
    "            .config(\"spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT\", \"1\")\n",
    "            .config(\"spark.yarn.appMasterEnv.ARROW_PRE_0_15_IPC_FORMAT\", \"1\")\n",
    "            .config(\n",
    "                \"spark.jars\",\n",
    "                \"hdfs:///jars/spark-bigquery-with-dependencies_2.11-0.17.3.jar,hdfs:///jars/spark-sql-kafka/net.jpountz.lz4_lz4-1.3.0.jar,hdfs:///jars/spark-sql-kafka/org.apache.kafka_kafka-clients-0.10.0.1.jar,hdfs:///jars/spark-sql-kafka/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.1.jar,hdfs:///jars/spark-sql-kafka/org.slf4j_slf4j-api-1.7.16.jar,hdfs:///jars/spark-sql-kafka/org.spark-project.spark_unused-1.0.0.jar,hdfs:///jars/spark-sql-kafka/org.xerial.snappy_snappy-java-1.1.2.6.jar\",\n",
    "            )\n",
    "            .config(\"spark.files\", SPARK_FILES)\n",
    "            .enableHiveSupport()\n",
    "            .getOrCreate()\n",
    "        )\n",
    "    else:\n",
    "        spark = (\n",
    "            SparkSession.builder.config(\"spark.app.name\", app_name)\n",
    "            .config(\"spark.driver.memory\", \"6g\")\n",
    "            .config(\"spark.executor.memory\", \"8g\")\n",
    "            .config(\"spark.shuffle.service.enabled\", \"true\")\n",
    "            .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "            .config(\"spark.dynamicAllocation.maxExecutors\", \"200\")\n",
    "            .config(\"spark.driver.maxResultSize\", \"6g\")\n",
    "            .config(\"spark.rpc.message.maxSize\", \"1024\")\n",
    "            .config(\"spark.yarn.queue\", queue)\n",
    "            .config(\"spark.ui.enabled\", \"false\")\n",
    "            .config(\"spark.port.maxRetries\", \"128\")\n",
    "            .config(\"spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT\", \"1\")\n",
    "            .config(\"spark.yarn.appMasterEnv.ARROW_PRE_0_15_IPC_FORMAT\", \"1\")\n",
    "            .config(\n",
    "                \"spark.jars\",\n",
    "                \"gs://external_libs/spark/jars/spark-bigquery-with-dependencies_2.11-0.16.1.jar,gs://external_libs/spark/jars/spark-sql-kafka/net.jpountz.lz4_lz4-1.3.0.jar,gs://external_libs/spark/jars/spark-sql-kafka/org.apache.kafka_kafka-clients-0.10.0.1.jar,gs://external_libs/spark/jars/spark-sql-kafka/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.1.jar,gs://external_libs/spark/jars/spark-sql-kafka/org.slf4j_slf4j-api-1.7.16.jar,gs://external_libs/spark/jars/spark-sql-kafka/org.spark-project.spark_unused-1.0.0.jar,gs://external_libs/spark/jars/spark-sql-kafka/org.xerial.snappy_snappy-java-1.1.2.6.jar\",\n",
    "            )\n",
    "            .config(\"spark.files\", SPARK_FILES)\n",
    "            .enableHiveSupport()\n",
    "            .getOrCreate()\n",
    "        )\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "    return spark\n",
    "\n",
    "spark = get_spark_for_kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dff1dd-86a9-4d2a-a8cd-16eaaa69b01a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_topics_set = \"('\" + \"','\".join(INPUT_TOPICS.split(',')) + \"')\"\n",
    "print(input_topics_set)\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT topic, partition, max(offset) as max_offset\n",
    "FROM {OUTPUT_TABLE}\n",
    "WHERE 1 = 1\n",
    "  AND topic IN {input_topics_set}\n",
    "  AND dt < '{DT}'\n",
    "group by topic, partition\n",
    "\"\"\"\n",
    "print(query)\n",
    "\n",
    "# get sink max offset\n",
    "max_offsets_df = spark.sql(query)\n",
    "max_offsets_pdf = max_offsets_df.toPandas()\n",
    "max_offsets_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a1118-ecdd-4bd8-9d75-8fb38657a7cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_offsets_pdf['starting_offset'] = max_offsets_pdf['max_offset'].map(lambda x: x+1)\n",
    "starting_offsets_pdf = max_offsets_pdf.drop(columns=['max_offset'])\n",
    "print(starting_offsets_pdf)\n",
    "\n",
    "starting_offsets = None\n",
    "\n",
    "if len(starting_offsets_pdf.index) > 0:\n",
    "    starting_offsets = starting_offsets_pdf.values.tolist()\n",
    "    print(starting_offsets)\n",
    "\n",
    "    from itertools import groupby\n",
    "    starting_offsets_dict = dict([(k, dict(map(lambda x: x[1:], list(g)))) for k, g in groupby(starting_offsets, lambda x: x[0])])\n",
    "    print(starting_offsets_dict)\n",
    "\n",
    "    import json\n",
    "    starting_offsets = json.dumps(starting_offsets_dict)\n",
    "\n",
    "print(starting_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa4fe8-9b32-41bc-acf6-3ddb6b526c65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_reader = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", BOOTSTRAP_SERVERS) \\\n",
    "  .option(\"subscribe\", INPUT_TOPICS) \\\n",
    "  .option(\"includeHeaders\", \"true\") \\\n",
    "  .option(\"enable.auto.commit\", \"false\")\n",
    "\n",
    "if starting_offsets is not None:\n",
    "    print(starting_offsets)\n",
    "    df_reader = df_reader \\\n",
    "        .option(\"startingOffsets\", starting_offsets)\\\n",
    "        .option(\"auto.offset.reset\", \"earliest\") \\\n",
    "        .option(\"failOnDataLoss\", \"false\")\n",
    "    \n",
    "if CONSUMER_CONFIGS != '':\n",
    "    config_tokens = CONSUMER_CONFIGS.split(',')\n",
    "    print(config_tokens)\n",
    "    \n",
    "    for token in config_tokens:\n",
    "        key_value = token.split('=')\n",
    "        print(key_value)\n",
    "        \n",
    "        df_reader = df_reader.option(f\"kafka.{key_value[0]}\", key_value[1])\n",
    "        \n",
    "df = df_reader.load()\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513eed5f-0af2-4f66-b665-5d9222abe58e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.registerTempTable(\"kafka\")\n",
    "\n",
    "query = f\"\"\"\n",
    "INSERT OVERWRITE TABLE {OUTPUT_TABLE}\n",
    "PARTITION (dt = \"{DT}\")\n",
    "SELECT\n",
    "    topic\n",
    "  , CAST(partition AS BIGINT) AS partition\n",
    "  , CAST(offset AS BIGINT) AS offset\n",
    "  , CAST(CAST(timestamp AS DOUBLE) * 1000 AS BIGINT) AS timestamp\n",
    "  , CAST(timestampType AS BIGINT) AS timestamp_type\n",
    "  , CAST(key AS STRING) AS key\n",
    "  , CAST(value AS STRING) AS value\n",
    "  , CAST(NULL AS STRING) AS headers\n",
    "  , \"{JOB}\" AS job\n",
    "FROM kafka\n",
    "\"\"\"\n",
    "print(query)\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff90222b-0a54-4960-8050-3cf3b8a83c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_output = spark.sql(f\"\"\"\n",
    "SELECT * FROM {OUTPUT_TABLE} WHERE dt = '{DT}'\n",
    "\"\"\")\n",
    "df_output.show()\n",
    "print(f\"df_output.count() = {df_output.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c119fc-8a79-4480-9756-14cb0e232b76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_output = spark.sql(f\"\"\"\n",
    "SELECT * FROM {OUTPUT_VIEW} WHERE dt = '{DT}'\n",
    "\"\"\")\n",
    "df_output.show()\n",
    "print(f\"df_output.count() = {df_output.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed2c230-8ee6-4211-ba0d-ebbaf4bac7f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = f'{OUTPUT_PATH}/dt={DT}'\n",
    "df_output.write.mode(\"overwrite\").parquet(path);\n",
    "\n",
    "!hadoop fs -ls {OUTPUT_PATH}\n",
    "!hadoop fs -ls {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a332190-45f3-4f23-b560-b88ed108bed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
