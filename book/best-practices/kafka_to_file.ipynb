{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b132e6-352b-49c1-b8a3-f286408f923b",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8f30f8-ebb6-48ac-887e-9ce17467cb85",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "BOOTSTRAP_SERVERS = 'b-2.emartai-streamhub-d.bnlyhx.c2.kafka.ap-northeast-2.amazonaws.com:9094,b-1.emartai-streamhub-d.bnlyhx.c2.kafka.ap-northeast-2.amazonaws.com:9094,b-3.emartai-streamhub-d.bnlyhx.c2.kafka.ap-northeast-2.amazonaws.com:9094'\n",
    "CONSUMER_CONFIGS = 'security.protocol=SSL,ssl.truststore.location=kafka-to-bigquery_certs_emartai-streamhub-datalake-cluster-stg_kafka.client.truststore.jks,ssl.truststore.password=changeit,ssl.keystore.location=kafka-to-bigquery_certs_emartai-streamhub-datalake-cluster-stg_kafka.client.keystore.jks,ssl.keystore.password=changeit,ssl.key.password=changeit'\n",
    "INPUT_TOPICS = 'tfactory-checkin'\n",
    "CONSUMER_GROUP = 'test-group-1'\n",
    "OUTPUT_TABLE = 'test_db.test_table'\n",
    "OUTPUT_VIEW = 'test_db.test_view'\n",
    "OUTPUT_PATH = 'hdfs:///tmp/test/test_table/'\n",
    "DT = '20210605'\n",
    "JOB = 'kafka-to-ye-hive-tfactory-checkin-test_db.test_table-20210605'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eaeaa6-f961-41f2-a2b8-934080edde4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HADOOP_USER_NAME'] = 'airflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c4a751-d53e-4760-b790-4a24dff27c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "consumer_config_list = []\n",
    "remote_file_list = []\n",
    "spark_file_list = []\n",
    "\n",
    "for config in CONSUMER_CONFIGS.split(','):\n",
    "    tokens = config.split('=')\n",
    "    key = tokens[0]\n",
    "    value = tokens[1]\n",
    "    if value.startswith('hdfs://'):\n",
    "        remote_file_list.append(value)\n",
    "        \n",
    "        filename = value.split('/').pop()\n",
    "        spark_file_list.append(filename)\n",
    "        \n",
    "        consumer_config_list.append(f'{key}={filename}')\n",
    "    else:\n",
    "        consumer_config_list.append(f'{key}={value}')\n",
    "        \n",
    "CONSUMER_CONFIGS = ','.join(consumer_config_list)\n",
    "print(CONSUMER_CONFIGS)\n",
    "\n",
    "REMOTE_FILES = ','.join(remote_file_list)\n",
    "print(REMOTE_FILES)\n",
    "%env REMOTE_FILES={REMOTE_FILES}\n",
    "\n",
    "SPARK_FILES = ','.join(spark_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3e437-b5fb-47ca-8284-99acd9a4541e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -x\n",
    "\n",
    "set -ex\n",
    "\n",
    "str=$REMOTE_FILES\n",
    "\n",
    "readarray -d , -t strarr <<<\"$str\"\n",
    "for (( n=0; n<${#strarr[*]}; n++ ))  \n",
    "do\n",
    "    echo ${strarr[n]}\n",
    "    hadoop fs -copyToLocal -f ${strarr[n]} .\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ee222-bc05-466a-8783-534857147373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydatafabric.vault_utils import get_secrets\n",
    "\n",
    "def get_spark_for_kafka(scale=0, queue=None):\n",
    "    import os\n",
    "    import uuid\n",
    "    import tempfile\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pydatafabric.vault_utils import get_secrets\n",
    "\n",
    "    tmp_uuid = str(uuid.uuid4())\n",
    "    app_name = f\"emart-{os.environ.get('USER', 'default')}-{tmp_uuid}\"\n",
    "    if not queue:\n",
    "        if \"JUPYTERHUB_USER\" in os.environ:\n",
    "            queue = \"dmig_eda\"\n",
    "        else:\n",
    "            queue = \"airflow_job\"\n",
    "    os.environ[\"ARROW_PRE_0_15_IPC_FORMAT\"] = \"1\"\n",
    "\n",
    "    key = get_secrets(\"gcp/emart-datafabric/dataflow\")[\"config\"]\n",
    "    key_file_name = tempfile.mkstemp()[1]\n",
    "    with open(key_file_name, \"wb\") as key_file:\n",
    "        key_file.write(key.encode())\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = key_file.name\n",
    "\n",
    "    if scale in [1, 2, 3, 4]:\n",
    "        spark = (\n",
    "            SparkSession.builder.config(\"spark.app.name\", app_name)\n",
    "            .config(\"spark.driver.memory\", f\"{scale*8}g\")\n",
    "            .config(\"spark.executor.memory\", f\"{scale*3}g\")\n",
    "            .config(\"spark.executor.instances\", f\"{scale*8}\")\n",
    "            .config(\"spark.driver.maxResultSize\", f\"{scale*4}g\")\n",
    "            .config(\"spark.rpc.message.maxSize\", \"1024\")\n",
    "            .config(\"spark.yarn.queue\", queue)\n",
    "            .config(\"spark.ui.enabled\", \"false\")\n",
    "            .config(\"spark.port.maxRetries\", \"128\")\n",
    "            .config(\"spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT\", \"1\")\n",
    "            .config(\"spark.yarn.appMasterEnv.ARROW_PRE_0_15_IPC_FORMAT\", \"1\")\n",
    "            .config(\n",
    "                \"spark.jars\",\n",
    "                \"hdfs:///jars/spark-bigquery-with-dependencies_2.11-0.17.3.jar,hdfs:///jars/spark-sql-kafka/net.jpountz.lz4_lz4-1.3.0.jar,hdfs:///jars/spark-sql-kafka/org.apache.kafka_kafka-clients-0.10.0.1.jar,hdfs:///jars/spark-sql-kafka/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.1.jar,hdfs:///jars/spark-sql-kafka/org.slf4j_slf4j-api-1.7.16.jar,hdfs:///jars/spark-sql-kafka/org.spark-project.spark_unused-1.0.0.jar,hdfs:///jars/spark-sql-kafka/org.xerial.snappy_snappy-java-1.1.2.6.jar\",\n",
    "            )\n",
    "            .config(\"spark.files\", SPARK_FILES)\n",
    "            .enableHiveSupport()\n",
    "            .getOrCreate()\n",
    "        )\n",
    "    else:\n",
    "        spark = (\n",
    "            SparkSession.builder.config(\"spark.app.name\", app_name)\n",
    "            .config(\"spark.driver.memory\", \"6g\")\n",
    "            .config(\"spark.executor.memory\", \"8g\")\n",
    "            .config(\"spark.shuffle.service.enabled\", \"true\")\n",
    "            .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "            .config(\"spark.dynamicAllocation.maxExecutors\", \"200\")\n",
    "            .config(\"spark.driver.maxResultSize\", \"6g\")\n",
    "            .config(\"spark.rpc.message.maxSize\", \"1024\")\n",
    "            .config(\"spark.yarn.queue\", queue)\n",
    "            .config(\"spark.ui.enabled\", \"false\")\n",
    "            .config(\"spark.port.maxRetries\", \"128\")\n",
    "            .config(\"spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT\", \"1\")\n",
    "            .config(\"spark.yarn.appMasterEnv.ARROW_PRE_0_15_IPC_FORMAT\", \"1\")\n",
    "            .config(\n",
    "                \"spark.jars\",\n",
    "                \"gs://external_libs/spark/jars/spark-bigquery-with-dependencies_2.11-0.16.1.jar,gs://external_libs/spark/jars/spark-sql-kafka/net.jpountz.lz4_lz4-1.3.0.jar,gs://external_libs/spark/jars/spark-sql-kafka/org.apache.kafka_kafka-clients-0.10.0.1.jar,gs://external_libs/spark/jars/spark-sql-kafka/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.1.jar,gs://external_libs/spark/jars/spark-sql-kafka/org.slf4j_slf4j-api-1.7.16.jar,gs://external_libs/spark/jars/spark-sql-kafka/org.spark-project.spark_unused-1.0.0.jar,gs://external_libs/spark/jars/spark-sql-kafka/org.xerial.snappy_snappy-java-1.1.2.6.jar\",\n",
    "            )\n",
    "            .config(\"spark.files\", SPARK_FILES)\n",
    "            .enableHiveSupport()\n",
    "            .getOrCreate()\n",
    "        )\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "    return spark\n",
    "\n",
    "spark = get_spark_for_kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dff1dd-86a9-4d2a-a8cd-16eaaa69b01a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_topics_set = \"('\" + \"','\".join(INPUT_TOPICS.split(',')) + \"')\"\n",
    "print(input_topics_set)\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT topic, partition, max(offset) as max_offset\n",
    "FROM {OUTPUT_TABLE}\n",
    "WHERE 1 = 1\n",
    "  AND topic IN {input_topics_set}\n",
    "  AND dt < '{DT}'\n",
    "group by topic, partition\n",
    "\"\"\"\n",
    "print(query)\n",
    "\n",
    "# get sink max offset\n",
    "max_offsets_df = spark.sql(query)\n",
    "max_offsets_pdf = max_offsets_df.toPandas()\n",
    "max_offsets_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a1118-ecdd-4bd8-9d75-8fb38657a7cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_offsets_pdf['starting_offset'] = max_offsets_pdf['max_offset'].map(lambda x: x+1)\n",
    "starting_offsets_pdf = max_offsets_pdf.drop(columns=['max_offset'])\n",
    "print(starting_offsets_pdf)\n",
    "\n",
    "starting_offsets = None\n",
    "\n",
    "if len(starting_offsets_pdf.index) > 0:\n",
    "    starting_offsets = starting_offsets_pdf.values.tolist()\n",
    "    print(starting_offsets)\n",
    "\n",
    "    from itertools import groupby\n",
    "    starting_offsets_dict = dict([(k, dict(map(lambda x: x[1:], list(g)))) for k, g in groupby(starting_offsets, lambda x: x[0])])\n",
    "    print(starting_offsets_dict)\n",
    "\n",
    "    import json\n",
    "    starting_offsets = json.dumps(starting_offsets_dict)\n",
    "\n",
    "print(starting_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa4fe8-9b32-41bc-acf6-3ddb6b526c65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_reader = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", BOOTSTRAP_SERVERS) \\\n",
    "  .option(\"subscribe\", INPUT_TOPICS) \\\n",
    "  .option(\"includeHeaders\", \"true\") \\\n",
    "  .option(\"enable.auto.commit\", \"false\")\n",
    "\n",
    "if starting_offsets is not None:\n",
    "    print(starting_offsets)\n",
    "    df_reader = df_reader \\\n",
    "        .option(\"startingOffsets\", starting_offsets)\\\n",
    "        .option(\"auto.offset.reset\", \"earliest\") \\\n",
    "        .option(\"failOnDataLoss\", \"false\")\n",
    "    \n",
    "if CONSUMER_CONFIGS != '':\n",
    "    config_tokens = CONSUMER_CONFIGS.split(',')\n",
    "    print(config_tokens)\n",
    "    \n",
    "    for token in config_tokens:\n",
    "        key_value = token.split('=')\n",
    "        print(key_value)\n",
    "        \n",
    "        df_reader = df_reader.option(f\"kafka.{key_value[0]}\", key_value[1])\n",
    "        \n",
    "df = df_reader.load()\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513eed5f-0af2-4f66-b665-5d9222abe58e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.registerTempTable(\"kafka\")\n",
    "\n",
    "query = f\"\"\"\n",
    "INSERT OVERWRITE TABLE {OUTPUT_TABLE}\n",
    "PARTITION (dt = \"{DT}\")\n",
    "SELECT\n",
    "    topic\n",
    "  , CAST(partition AS BIGINT) AS partition\n",
    "  , CAST(offset AS BIGINT) AS offset\n",
    "  , CAST(CAST(timestamp AS DOUBLE) * 1000 AS BIGINT) AS timestamp\n",
    "  , CAST(timestampType AS BIGINT) AS timestamp_type\n",
    "  , CAST(key AS STRING) AS key\n",
    "  , CAST(value AS STRING) AS value\n",
    "  , CAST(NULL AS STRING) AS headers\n",
    "  , \"{JOB}\" AS job\n",
    "FROM kafka\n",
    "\"\"\"\n",
    "print(query)\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff90222b-0a54-4960-8050-3cf3b8a83c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_output = spark.sql(f\"\"\"\n",
    "SELECT * FROM {OUTPUT_TABLE} WHERE dt = '{DT}'\n",
    "\"\"\")\n",
    "df_output.show()\n",
    "print(f\"df_output.count() = {df_output.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c119fc-8a79-4480-9756-14cb0e232b76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_output = spark.sql(f\"\"\"\n",
    "SELECT * FROM {OUTPUT_VIEW} WHERE dt = '{DT}'\n",
    "\"\"\")\n",
    "df_output.show()\n",
    "print(f\"df_output.count() = {df_output.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed2c230-8ee6-4211-ba0d-ebbaf4bac7f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = f'{OUTPUT_PATH}/dt={DT}'\n",
    "df_output.write.mode(\"overwrite\").parquet(path);\n",
    "\n",
    "!hadoop fs -ls {OUTPUT_PATH}\n",
    "!hadoop fs -ls {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a332190-45f3-4f23-b560-b88ed108bed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}