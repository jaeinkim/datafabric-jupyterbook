{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b132e6-352b-49c1-b8a3-f286408f923b",
   "metadata": {},
   "source": [
    "### pyspark read kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74ba4565-2910-4a46-8d1b-f62bdcafa8d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T05:55:15.231867Z",
     "iopub.status.busy": "2022-10-13T05:55:15.231488Z",
     "iopub.status.idle": "2022-10-13T05:55:15.234992Z",
     "shell.execute_reply": "2022-10-13T05:55:15.234444Z",
     "shell.execute_reply.started": "2022-10-13T05:55:15.231844Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT = \"emart-datafabric\"\n",
    "DATASET = \"common_dev\"\n",
    "TABLE = \"dfm_sample_eapp_data\"\n",
    "LIMIT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c8f30f8-ebb6-48ac-887e-9ce17467cb85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T05:56:33.232241Z",
     "iopub.status.busy": "2022-10-13T05:56:33.231833Z",
     "iopub.status.idle": "2022-10-13T05:56:33.235390Z",
     "shell.execute_reply": "2022-10-13T05:56:33.234885Z",
     "shell.execute_reply.started": "2022-10-13T05:56:33.232202Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "BOOTSTRAP_SERVERS = \"datafabric-kafka-kafka-bootstrap.kafka-farm.svc.cluster.local:9092\"\n",
    "TOPICS = 'test'\n",
    "CONSUMER_GROUP = 'test-datafabric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a88909d-2205-4ca6-b7f9-caad53c1ead4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T05:55:16.124746Z",
     "iopub.status.busy": "2022-10-13T05:55:16.124313Z",
     "iopub.status.idle": "2022-10-13T05:55:16.164515Z",
     "shell.execute_reply": "2022-10-13T05:55:16.163946Z",
     "shell.execute_reply.started": "2022-10-13T05:55:16.124720Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydatafabric.ye import get_spark\n",
    "from pydatafabric.gcp import bq_to_df\n",
    "\n",
    "extra_jars = [\"kafka-clients-2.8.1.jar\", \"slf4j-api-1.7.30.jar\", \"lz4-java-1.7.1.jar\", \"snappy-java-1.1.8.1.jar\", \"spark-sql-kafka-0-10_2.12-3.1.2.jar\", \"unused-1.0.0.jar\", \"spark-token-provider-kafka-0-10_2.12-3.3.0.jar\"]\n",
    "extra_jars = [\"gs://emart-datafabric-resources/jars/spark-kafka-jars/\" + data for data in extra_jars]\n",
    "extra_jars = \",\".join(extra_jars)\n",
    "\n",
    "spark = get_spark(extra_jars=extra_jars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ed2ba13-a286-44bc-b589-f946d03cb472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T05:55:19.151531Z",
     "iopub.status.busy": "2022-10-13T05:55:19.151082Z",
     "iopub.status.idle": "2022-10-13T05:55:19.156105Z",
     "shell.execute_reply": "2022-10-13T05:55:19.155546Z",
     "shell.execute_reply.started": "2022-10-13T05:55:19.151503Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hdfs:///jars/spark-bigquery-with-dependencies_2.12-latest.jar,gs://emart-datafabric-resources/jars/spark-kafka-jars/kafka-clients-2.8.1.jar,gs://emart-datafabric-resources/jars/spark-kafka-jars/slf4j-api-1.7.30.jar,gs://emart-datafabric-resources/jars/spark-kafka-jars/lz4-java-1.7.1.jar,gs://emart-datafabric-resources/jars/spark-kafka-jars/snappy-java-1.1.8.1.jar,gs://emart-datafabric-resources/jars/spark-kafka-jars/spark-sql-kafka-0-10_2.12-3.1.2.jar,gs://emart-datafabric-resources/jars/spark-kafka-jars/unused-1.0.0.jar,gs://emart-datafabric-resources/jars/spark-kafka-jars/spark-token-provider-kafka-0-10_2.12-3.3.0.jar'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.jars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c276770c-fd15-4044-a913-ce20f716f375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T05:55:19.810367Z",
     "iopub.status.busy": "2022-10-13T05:55:19.809977Z",
     "iopub.status.idle": "2022-10-13T05:55:24.607231Z",
     "shell.execute_reply": "2022-10-13T05:55:24.606628Z",
     "shell.execute_reply.started": "2022-10-13T05:55:19.810342Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = bq_to_df(f\"\"\"\n",
    "    select review_id as key, comments as value\n",
    "    from  `{PROJECT}.{DATASET}.{TABLE}`\n",
    "    where comments != ''\n",
    "    limit {LIMIT}\n",
    "\"\"\", spark_session=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b125de5-8110-4453-b71b-f7efabdd6a40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T05:55:25.397860Z",
     "iopub.status.busy": "2022-10-13T05:55:25.397479Z",
     "iopub.status.idle": "2022-10-13T05:55:28.626075Z",
     "shell.execute_reply": "2022-10-13T05:55:28.625512Z",
     "shell.execute_reply.started": "2022-10-13T05:55:25.397835Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------------------+\n",
      "|             key|                             value|\n",
      "+----------------+----------------------------------+\n",
      "|2208170324332372|      천도 복숭아  싱싱한게 좋아요|\n",
      "|2208170545233407|    새콤하니 그럭저럭 먹을만 해요.|\n",
      "|2208170024210318| 맛도 좋고 싱싱해서 좋았습니다....|\n",
      "|2208171959293135|          맛잇아요 신선해서 조아요|\n",
      "|2208170938372306|             달고 맛있어요........|\n",
      "|2208172122122434|     골드키위 매일  하나씩 먹어요.|\n",
      "|2208170326197242|         상큼한 포모사 자두 좋아요|\n",
      "|2208171622152619|             자두 싱싱하고 달아요~|\n",
      "|2208171215499613|요즘 비많이와서 사까마까하다 샀...|\n",
      "|2208171137018485|                약간 단맛이 덜해요|\n",
      "+----------------+----------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ac3935c-d7bc-444a-8566-861601403d75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T05:55:30.295741Z",
     "iopub.status.busy": "2022-10-13T05:55:30.295355Z",
     "iopub.status.idle": "2022-10-13T05:55:30.310079Z",
     "shell.execute_reply": "2022-10-13T05:55:30.309550Z",
     "shell.execute_reply.started": "2022-10-13T05:55:30.295717Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2727d5-b030-487f-bbf3-ab94379dbd2b",
   "metadata": {},
   "source": [
    "### Writing the output of Batch Queries to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28fca6b5-e31b-4ede-86f9-d58c687781a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T05:55:37.054805Z",
     "iopub.status.busy": "2022-10-13T05:55:37.054396Z",
     "iopub.status.idle": "2022-10-13T05:55:39.309303Z",
     "shell.execute_reply": "2022-10-13T05:55:39.308381Z",
     "shell.execute_reply.started": "2022-10-13T05:55:37.054776Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/13 14:55:37 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2) (yellowelephant-w-1 executor 1): org.apache.kafka.common.KafkaException: Failed to construct kafka producer\n",
      "\tat org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:440)\n",
      "\tat org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:291)\n",
      "\tat org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:274)\n",
      "\tat org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool.createKafkaProducer(InternalKafkaProducerPool.scala:136)\n",
      "\tat org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool.$anonfun$acquire$1(InternalKafkaProducerPool.scala:83)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool.acquire(InternalKafkaProducerPool.scala:82)\n",
      "\tat org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool$.acquire(InternalKafkaProducerPool.scala:198)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:49)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:72)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:73)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers\n",
      "\tat org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:89)\n",
      "\tat org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:48)\n",
      "\tat org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:414)\n",
      "\t... 24 more\n",
      "\n",
      "22/10/13 14:55:38 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o156.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5) (yellowelephant-w-1 executor 1): org.apache.kafka.common.KafkaException: Failed to construct kafka producer\n\tat org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:440)\n\tat org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:291)\n\tat org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:274)\n\tat org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool.createKafkaProducer(InternalKafkaProducerPool.scala:136)\n\tat org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool.$anonfun$acquire$1(InternalKafkaProducerPool.scala:83)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool.acquire(InternalKafkaProducerPool.scala:82)\n\tat org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool$.acquire(InternalKafkaProducerPool.scala:198)\n\tat org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:49)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:72)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:73)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers\n\tat org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:89)\n\tat org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:48)\n\tat org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:414)\n\t... 24 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2259)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2208)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2207)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2446)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2388)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2377)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2225)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2244)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2269)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.write(KafkaWriter.scala:70)\n\tat org.apache.spark.sql.kafka010.KafkaSourceProvider.createRelation(KafkaSourceProvider.scala:180)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.kafka.common.KafkaException: Failed to construct kafka producer\n\tat org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:440)\n\tat org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:291)\n\tat org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:274)\n\tat org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool.createKafkaProducer(InternalKafkaProducerPool.scala:136)\n\tat org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool.$anonfun$acquire$1(InternalKafkaProducerPool.scala:83)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool.acquire(InternalKafkaProducerPool.scala:82)\n\tat org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool$.acquire(InternalKafkaProducerPool.scala:198)\n\tat org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:49)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:72)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:73)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers\n\tat org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:89)\n\tat org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:48)\n\tat org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:414)\n\t... 24 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(key AS STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(value AS STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;241m.\u001b[39mwrite \\\n\u001b[1;32m      3\u001b[0m   \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.bootstrap.servers\u001b[39m\u001b[38;5;124m\"\u001b[39m, BOOTSTRAP_SERVERS) \\\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m, TOPICS) \\\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:1107\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1107\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o156.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5) (yellowelephant-w-1 executor 1): org.apache.kafka.common.KafkaException: Failed to construct kafka producer\n\tat org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:440)\n\tat org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:291)\n\tat org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:274)\n\tat org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool.createKafkaProducer(InternalKafkaProducerPool.scala:136)\n\tat org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool.$anonfun$acquire$1(InternalKafkaProducerPool.scala:83)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool.acquire(InternalKafkaProducerPool.scala:82)\n\tat org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool$.acquire(InternalKafkaProducerPool.scala:198)\n\tat org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:49)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:72)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:73)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers\n\tat org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:89)\n\tat org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:48)\n\tat org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:414)\n\t... 24 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2259)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2208)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2207)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2446)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2388)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2377)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2225)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2244)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2269)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.write(KafkaWriter.scala:70)\n\tat org.apache.spark.sql.kafka010.KafkaSourceProvider.createRelation(KafkaSourceProvider.scala:180)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.kafka.common.KafkaException: Failed to construct kafka producer\n\tat org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:440)\n\tat org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:291)\n\tat org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:274)\n\tat org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool.createKafkaProducer(InternalKafkaProducerPool.scala:136)\n\tat org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool.$anonfun$acquire$1(InternalKafkaProducerPool.scala:83)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool.acquire(InternalKafkaProducerPool.scala:82)\n\tat org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool$.acquire(InternalKafkaProducerPool.scala:198)\n\tat org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:49)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:72)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:73)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers\n\tat org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:89)\n\tat org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:48)\n\tat org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:414)\n\t... 24 more\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .write \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", BOOTSTRAP_SERVERS) \\\n",
    "  .option(\"topic\", TOPICS) \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01595bf5-6405-463c-a433-c7ca1d58d6c8",
   "metadata": {},
   "source": [
    "### Reading Data from Kafka Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa4fe8-9b32-41bc-acf6-3ddb6b526c65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T06:11:30.453101Z",
     "iopub.status.busy": "2022-10-12T06:11:30.452305Z",
     "iopub.status.idle": "2022-10-12T06:11:30.757742Z",
     "shell.execute_reply": "2022-10-12T06:11:30.756952Z",
     "shell.execute_reply.started": "2022-10-12T06:11:30.453069Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "starting_offsets = {TOPICS:{\"0\":1}} # TOPIC의 id가 0인 파티션의 1번째 Offsets의 데이터\n",
    "starting_offsets = json.dumps(starting_offsets)\n",
    "\n",
    "\n",
    "batch_df_reader = spark.read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"subscribe\", TOPICS) \\\n",
    "  .option(\"kafka.bootstrap.servers\", BOOTSTRAP_SERVERS) \\\n",
    "  .option(\"kafka.security.protocol\", \"SSL\") \\\n",
    "  .option(\"startingOffsets\", starting_offsets)\\\n",
    "  .option(\"auto.offset.reset\", \"earliest\") \\\n",
    "  .option(\"failOnDataLoss\", \"false\") \\\n",
    "  .option(\"enable.auto.commit\", \"false\")\n",
    "\n",
    "batch_df = batch_df_reader.load()\n",
    "batch_df = batch_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"topic\", \"partition\", \"offset\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6eb55-2779-47fd-b02c-ed2e2f2f7dda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T06:11:32.468480Z",
     "iopub.status.busy": "2022-10-12T06:11:32.468080Z",
     "iopub.status.idle": "2022-10-12T06:11:32.471859Z",
     "shell.execute_reply": "2022-10-12T06:11:32.471331Z",
     "shell.execute_reply.started": "2022-10-12T06:11:32.468456Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfbd39a-1d04-4c7d-b08e-1cbddd117477",
   "metadata": {},
   "source": [
    "### Creating a Kafka Source for Streaming Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3119437-0c85-4c60-8c0d-8576d1dcaa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", BOOTSTRAP_SERVERS) \\\n",
    "  .option(\"subscribe\", TOPICS) \\\n",
    "  .load()\n",
    "\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208cb705-ab8f-4987-b593-2c3f0ad64ae8",
   "metadata": {},
   "source": [
    "### Creating a Kafka Sink for Streaming Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a146d84-76f7-4910-9710-886c4f26bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", BOOTSTRAP_SERVERS) \\\n",
    "  .option(\"topic\", TOPICS) \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a332190-45f3-4f23-b560-b88ed108bed7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T05:52:59.698451Z",
     "iopub.status.busy": "2022-10-12T05:52:59.698058Z",
     "iopub.status.idle": "2022-10-12T05:53:00.275351Z",
     "shell.execute_reply": "2022-10-12T05:53:00.274746Z",
     "shell.execute_reply.started": "2022-10-12T05:52:59.698426Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccd6b4f-5962-4f77-85f6-fb11eebba3d3",
   "metadata": {},
   "source": [
    "### python kafka package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7afbb77-147b-4c47-9bc1-e2923ac1d412",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T07:19:39.398914Z",
     "iopub.status.busy": "2022-10-12T07:19:39.398567Z",
     "iopub.status.idle": "2022-10-12T07:19:41.642893Z",
     "shell.execute_reply": "2022-10-12T07:19:41.642121Z",
     "shell.execute_reply.started": "2022-10-12T07:19:39.398885Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kafka-python in /home/x223726/.local/lib/python3.8/site-packages (2.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99817987-c89e-4f53-91f6-093ab0ace444",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T07:33:00.592245Z",
     "iopub.status.busy": "2022-10-12T07:33:00.591859Z",
     "iopub.status.idle": "2022-10-12T07:33:01.139544Z",
     "shell.execute_reply": "2022-10-12T07:33:01.138985Z",
     "shell.execute_reply.started": "2022-10-12T07:33:00.592202Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from json import dumps\n",
    "\n",
    "producer = KafkaProducer(acks=0,\n",
    "                         compression_type='gzip',\n",
    "                         bootstrap_servers=BOOTSTRAP_SERVERS,\n",
    "                         key_serializer=str.encode,\n",
    "                         value_serializer=lambda x: dumps(x).encode('utf-8')\n",
    "                         )\n",
    "\n",
    "for i in range(1000):\n",
    "    producer.send(TOPICS, key= 'key_' + str(i), value='value_' + str(i))\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f4ac7-37ca-409e-b535-0b59d4613c0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T08:04:01.216260Z",
     "iopub.status.busy": "2022-10-12T08:04:01.215876Z",
     "iopub.status.idle": "2022-10-12T08:04:04.646188Z",
     "shell.execute_reply": "2022-10-12T08:04:04.645596Z",
     "shell.execute_reply.started": "2022-10-12T08:04:01.216236Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[begin] get consumer list\n",
      "[end] get consumer list\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from json import loads\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    TOPICS,\n",
    "    bootstrap_servers=BOOTSTRAP_SERVERS,\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=True,\n",
    "    group_id=CONSUMER_GROUP,    \n",
    "    value_deserializer=lambda x: loads(x.decode('utf-8')),\n",
    "    consumer_timeout_ms=1000\n",
    ")\n",
    "\n",
    "\n",
    "# consumer 리스트를 가져온다\n",
    "print('[begin] get consumer list')\n",
    "for message in consumer:\n",
    "    print(\"Topic: %s, Partition: %d, Offset: %d, Key: %s, Value: %s\" % (\n",
    "        message.topic, message.partition, message.offset, message.key, message.value\n",
    "    ))\n",
    "print('[end] get consumer list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77c2991-6463-4592-aeb9-b49b4d7ff3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
