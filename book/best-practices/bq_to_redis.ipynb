{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff651f27-35d9-4ab7-94fa-8ab2cc59bdcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T03:06:36.555803Z",
     "iopub.status.busy": "2022-10-12T03:06:36.555413Z",
     "iopub.status.idle": "2022-10-12T03:06:36.561677Z",
     "shell.execute_reply": "2022-10-12T03:06:36.561161Z",
     "shell.execute_reply.started": "2022-10-12T03:06:36.555743Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT = \"emart-datafabric\"\n",
    "DATASET = \"common_dev\"\n",
    "TABLE = \"dfm_sample_eapp_data\"\n",
    "DT = \"2022-08-30\"\n",
    "LIMIT = 30\n",
    "\n",
    "REDIS_INFO = {\n",
    "    \"host\": \"redis-master.redis-farm.svc.cluster.local\",\n",
    "    \"port\": \"6379\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cfd45ed-f648-4010-bbec-acc840039717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T03:06:37.137638Z",
     "iopub.status.busy": "2022-10-12T03:06:37.137272Z",
     "iopub.status.idle": "2022-10-12T03:06:54.031584Z",
     "shell.execute_reply": "2022-10-12T03:06:54.030922Z",
     "shell.execute_reply.started": "2022-10-12T03:06:37.137618Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/12 12:06:41 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "22/10/12 12:06:41 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "22/10/12 12:06:41 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/10/12 12:06:41 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "from pydatafabric.gcp import bq_to_df\n",
    "from pydatafabric.ye import get_spark\n",
    "\n",
    "spark = get_spark(extra_jars=\"gs://emart-datafabric-resources/jars/spark-redis_2.12-3.1.0-jar-with-dependencies.jar\")\n",
    "spark.conf.set(\"spark.redis.host\", REDIS_INFO[\"host\"])\n",
    "spark.conf.set(\"spark.redis.port\", REDIS_INFO[\"port\"])\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "df = bq_to_df(f\"\"\"\n",
    "    select  *\n",
    "    from  `{PROJECT}.{DATASET}.{TABLE}`\n",
    "    WHERE dt = '{DT}'\n",
    "    limit {LIMIT}\n",
    "\"\"\", spark_session=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54ce5b2b-53fd-44ba-9023-52ba8952512f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T03:06:54.708064Z",
     "iopub.status.busy": "2022-10-12T03:06:54.707478Z",
     "iopub.status.idle": "2022-10-12T03:06:54.717030Z",
     "shell.execute_reply": "2022-10-12T03:06:54.716534Z",
     "shell.execute_reply.started": "2022-10-12T03:06:54.708034Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hdfs:///jars/spark-bigquery-with-dependencies_2.12-latest.jar,gs://emart-datafabric-resources/jars/spark-redis_2.12-3.1.0-jar-with-dependencies.jar'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.jars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7b6b116-f6b3-4d70-84c5-571cd08ca43b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T03:06:56.250602Z",
     "iopub.status.busy": "2022-10-12T03:06:56.250068Z",
     "iopub.status.idle": "2022-10-12T03:07:01.736786Z",
     "shell.execute_reply": "2022-10-12T03:07:01.736202Z",
     "shell.execute_reply.started": "2022-10-12T03:06:56.250568Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/12 12:06:58 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+---------+---------+---+----------------+--------+-------------+-----+----------------------------------+---------------+-------------+---------------------------------+----------+----------+-----------+-----------+----------+----------+-------------+-------------+-------------+-------------+-------------+------------------+-------+-------+-------------+------------+--------------+-----------+-----------+---------+----------+-----------+--------------------+-------------+\n",
      "|       review_id|             cust_id|gender_cd|gender_nm|age|agrde_cd_10_unit|store_cd|     store_nm|score|                          comments|comments_length|       sku_cd|                           sku_nm|  order_dt|        dt|prdt_cat_cd|prdt_cat_nm|prdt_di_cd|prdt_di_nm|prdt_gcode_cd|prdt_gcode_nm|prdt_mcode_cd|prdt_mcode_nm|prdt_dcode_cd|     prdt_dcode_nm|area_cd|area_nm|    longitude|    latitude|comments_point|image_point|thumb_point|action_cd|blind_flag|active_flag|            tag_list|avg_tag_score|\n",
      "+----------------+--------------------+---------+---------+---+----------------+--------+-------------+-----+----------------------------------+---------------+-------------+---------------------------------+----------+----------+-----------+-----------+----------+----------+-------------+-------------+-------------+-------------+-------------+------------------+-------+-------+-------------+------------+--------------+-----------+-----------+---------+----------+-----------+--------------------+-------------+\n",
      "|2208301136446921|28d1c69d1a2a3cc6b...|        2|       여| 54|              50|    1000|이마트 창동점|   10| 맛과 신선도 용량은 만족하지만 ...|             29|8809069301064|        Dole 스위티오 바나나 10CP|2022-08-29|2022-08-30|         11|       과일|        10| 신선1담당|          111|     수입과일|         9511|       바나나|         5835|    바나나(고산지)|     00|   서울|127.046690683|37.651608368|            10|         10|          0|      003|         N|          Y|111::1::10,111::2...|  9.000000000|\n",
      "|2208301348410479|0dacb55eb9f6d77b3...|        2|       여| 63|              60|    1000|이마트 창동점|   10|    여름에 먹는 귤맛이  끝내주네요|             17|2500000034114|파머스픽 당도선별 하우스감귤 1...|2022-08-29|2022-08-30|         11|       과일|        10| 신선1담당|          112| 국산연중과일|         9503|         감귤|         0533|      감귤(하우스)|     00|   서울|127.046690683|37.651608368|            10|         10|          0|      003|         N|          Y|112::1::8,112::2:...|  8.000000000|\n",
      "|2208300923271031|03c4f22b4d5b99891...|        1|       남| 36|              30|    1000|이마트 창동점|   10|        저렴하고 맛있어요!!!!!!!!!|             18|8809676682501|     (SF)피코크 된장찌개 요리재료|2022-08-29|2022-08-30|         13|       채소|        10| 신선1담당|          130|     간편채소|         0438|PEACOCK밀키트|         0263|P)한식(국탕)밀키트|     00|   서울|127.046690683|37.651608368|            10|         10|          0|      003|         N|          Y|130::1::10,130::2...| 10.000000000|\n",
      "|2208301122560242|28d1c69d1a2a3cc6b...|        2|       여| 54|              50|    1000|이마트 창동점|   10| 가격 할인과 함께 신선도와 용량...|             27|1305650000000|    무항생제더느림+돈앞다리수육용|2022-08-29|2022-08-30|         14|       축산|        11| 신선2담당|          141|         돈육|         1580|   브랜드돈육|         5293|    브랜드돈앞다리|     00|   서울|127.046690683|37.651608368|            10|         10|          0|      003|         N|          Y|141::1::10,141::2...| 10.000000000|\n",
      "|2208301122086191|28d1c69d1a2a3cc6b...|        2|       여| 54|              50|    1000|이마트 창동점|   10| 가격 할인은 아니지만 신선도와 ...|             27|2429230000000|           (대)국내산삼겹살구이용|2022-08-29|2022-08-30|         14|       축산|        11| 신선2담당|          141|         돈육|         9571|     일반돈육|         5299|            삼겹살|     00|   서울|127.046690683|37.651608368|            10|         10|          0|      003|         N|          Y|141::1::10,141::2...| 10.000000000|\n",
      "|2208301530516339|30ddeda2bc65555db...|        2|       여| 57|              50|    1000|이마트 창동점|   10|  구워서  소금에 살짝 찍어 맛있...|             24|1264120000000|           달링다운와규냉장설깃살|2022-08-29|2022-08-30|         14|       축산|        11| 신선2담당|          143|       수입육|         9670| 호주산브랜드|         0548|  호주산브랜드설도|     00|   서울|127.046690683|37.651608368|            10|          0|          0|      001|         N|          Y|143::1::8,143::2:...|  8.000000000|\n",
      "|2208300828497973|1305e6f03458d4cd2...|        2|       여| 74|              70|    1000|이마트 창동점|   10|       가격이 더 저렴했으면 합니다|             15|2500000277924|           (KG)양념등심돈까스850g|2022-08-13|2022-08-30|         14|       축산|        11| 신선2담당|          144|   양념가공육|         0355|   간편육가공|         2281|         직)까스류|     00|   서울|127.046690683|37.651608368|            10|          0|          0|      001|         N|          Y|144::1::8,144::2:...|  8.000000000|\n",
      "|2208301430288840|02b7d95c81e0a0b16...|        2|       여| 62|              60|    1000|이마트 창동점|   10| 담백하고 비리지 않은 가자미 국...|             32|2116040000000|     손질 용가자미(중, 60미, 1...|2022-08-29|2022-08-30|         15|       수산|        11| 신선2담당|          153|     냉동생선|         9631| 냉동규격생선|         1695|      냉동벌크행사|     00|   서울|127.046690683|37.651608368|            10|         10|          0|      003|         N|          Y|153::1::6,153::2:...|  6.000000000|\n",
      "|2208300006452516|239ec5061e62ff2d3...|        2|       여| 87|              70|    1000|이마트 창동점|   10|        오픈런까지 한 보람은 좀 ..|             16|1179140000000|                     햇꽃게(빙장)|2022-08-27|2022-08-30|         15|       수산|        11| 신선2담당|          154|    갑각/패류|         1890|         꽃게|         0014|          시즌꽃게|     00|   서울|127.046690683|37.651608368|            10|         10|          0|      003|         N|          Y|154::1::10,154::2...| 10.000000000|\n",
      "|2208300002085208|239ec5061e62ff2d3...|        2|       여| 87|              70|    1000|이마트 창동점|   10|커서 사용할때 주의해야겠지만 저...|             23|7502007293209|          산루카스 아보카도오일1L|2022-08-27|2022-08-30|         21|      가공A|        30|  가공담당|          311|     소스오일|         3779|       유지류|         7799|      아보카도오일|     00|   서울|127.046690683|37.651608368|            10|         10|          0|      003|         N|          Y|311::1::10,311::2...| 10.000000000|\n",
      "+----------------+--------------------+---------+---------+---+----------------+--------+-------------+-----+----------------------------------+---------------+-------------+---------------------------------+----------+----------+-----------+-----------+----------+----------+-------------+-------------+-------------+-------------+-------------+------------------+-------+-------+-------------+------------+--------------+-----------+-----------+---------+----------+-----------+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd9c3d22-3a76-4d13-b6ca-d95fb1f85f0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T03:07:04.744245Z",
     "iopub.status.busy": "2022-10-12T03:07:04.743869Z",
     "iopub.status.idle": "2022-10-12T03:07:04.935616Z",
     "shell.execute_reply": "2022-10-12T03:07:04.935039Z",
     "shell.execute_reply.started": "2022-10-12T03:07:04.744208Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(f\"temp_{TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "162c3e63-5877-45a1-946a-825114beea73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T03:07:05.215412Z",
     "iopub.status.busy": "2022-10-12T03:07:05.215023Z",
     "iopub.status.idle": "2022-10-12T03:07:09.066719Z",
     "shell.execute_reply": "2022-10-12T03:07:09.066148Z",
     "shell.execute_reply.started": "2022-10-12T03:07:05.215387Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+---------+---------+---+----------------+--------+-------------+-----+---------------------------------+---------------+-------------+-------------------------+----------+----------+-----------+-----------+----------+----------+-------------+-------------+-------------+-------------+-------------+--------------+-------+-------+-------------+------------+--------------+-----------+-----------+---------+----------+-----------+--------------------+-------------+\n",
      "|       review_id|             cust_id|gender_cd|gender_nm|age|agrde_cd_10_unit|store_cd|     store_nm|score|                         comments|comments_length|       sku_cd|                   sku_nm|  order_dt|        dt|prdt_cat_cd|prdt_cat_nm|prdt_di_cd|prdt_di_nm|prdt_gcode_cd|prdt_gcode_nm|prdt_mcode_cd|prdt_mcode_nm|prdt_dcode_cd| prdt_dcode_nm|area_cd|area_nm|    longitude|    latitude|comments_point|image_point|thumb_point|action_cd|blind_flag|active_flag|            tag_list|avg_tag_score|\n",
      "+----------------+--------------------+---------+---------+---+----------------+--------+-------------+-----+---------------------------------+---------------+-------------+-------------------------+----------+----------+-----------+-----------+----------+----------+-------------+-------------+-------------+-------------+-------------+--------------+-------+-------+-------------+------------+--------------+-----------+-----------+---------+----------+-----------+--------------------+-------------+\n",
      "|2208301136446921|28d1c69d1a2a3cc6b...|        2|       여| 54|              50|    1000|이마트 창동점|   10|맛과 신선도 용량은 만족하지만 ...|             29|8809069301064|Dole 스위티오 바나나 10CP|2022-08-29|2022-08-30|         11|       과일|        10| 신선1담당|          111|     수입과일|         9511|       바나나|         5835|바나나(고산지)|     00|   서울|127.046690683|37.651608368|            10|         10|          0|      003|         N|          Y|111::1::10,111::2...|  9.000000000|\n",
      "+----------------+--------------------+---------+---------+---+----------------+--------+-------------+-----+---------------------------------+---------------+-------------+-------------------------+----------+----------+-----------+-----------+----------+----------+-------------+-------------+-------------+-------------+-------------+--------------+-------+-------+-------------+------------+--------------+-----------+-----------+---------+----------+-----------+--------------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql(\n",
    "    f'select * from temp_{TABLE} limit 1')\n",
    "\n",
    "result_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83fc614c-2ce0-44e8-978c-0e1c837f5e8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T03:07:11.150518Z",
     "iopub.status.busy": "2022-10-12T03:07:11.150125Z",
     "iopub.status.idle": "2022-10-12T03:07:11.164837Z",
     "shell.execute_reply": "2022-10-12T03:07:11.164319Z",
     "shell.execute_reply.started": "2022-10-12T03:07:11.150494Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- cust_id: string (nullable = true)\n",
      " |-- gender_cd: string (nullable = true)\n",
      " |-- gender_nm: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- agrde_cd_10_unit: string (nullable = true)\n",
      " |-- store_cd: string (nullable = true)\n",
      " |-- store_nm: string (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      " |-- comments_length: long (nullable = true)\n",
      " |-- sku_cd: string (nullable = true)\n",
      " |-- sku_nm: string (nullable = true)\n",
      " |-- order_dt: date (nullable = true)\n",
      " |-- dt: date (nullable = true)\n",
      " |-- prdt_cat_cd: string (nullable = true)\n",
      " |-- prdt_cat_nm: string (nullable = true)\n",
      " |-- prdt_di_cd: string (nullable = true)\n",
      " |-- prdt_di_nm: string (nullable = true)\n",
      " |-- prdt_gcode_cd: string (nullable = true)\n",
      " |-- prdt_gcode_nm: string (nullable = true)\n",
      " |-- prdt_mcode_cd: string (nullable = true)\n",
      " |-- prdt_mcode_nm: string (nullable = true)\n",
      " |-- prdt_dcode_cd: string (nullable = true)\n",
      " |-- prdt_dcode_nm: string (nullable = true)\n",
      " |-- area_cd: string (nullable = true)\n",
      " |-- area_nm: string (nullable = true)\n",
      " |-- longitude: decimal(38,9) (nullable = true)\n",
      " |-- latitude: decimal(38,9) (nullable = true)\n",
      " |-- comments_point: long (nullable = true)\n",
      " |-- image_point: long (nullable = true)\n",
      " |-- thumb_point: long (nullable = true)\n",
      " |-- action_cd: string (nullable = true)\n",
      " |-- blind_flag: string (nullable = true)\n",
      " |-- active_flag: string (nullable = true)\n",
      " |-- tag_list: string (nullable = true)\n",
      " |-- avg_tag_score: decimal(38,9) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bece4a1-6425-4e3c-a28e-acbfb51ace59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T03:07:15.445182Z",
     "iopub.status.busy": "2022-10-12T03:07:15.444821Z",
     "iopub.status.idle": "2022-10-12T03:07:16.978914Z",
     "shell.execute_reply": "2022-10-12T03:07:16.978054Z",
     "shell.execute_reply.started": "2022-10-12T03:07:15.445159Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o118.save.\n: redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool\n\tat redis.clients.jedis.util.Pool.getResource(Pool.java:84)\n\tat redis.clients.jedis.JedisPool.getResource(JedisPool.java:377)\n\tat com.redislabs.provider.redis.ConnectionPool$.connect(ConnectionPool.scala:35)\n\tat com.redislabs.provider.redis.RedisEndpoint.connect(RedisConfig.scala:94)\n\tat com.redislabs.provider.redis.RedisConfig.clusterEnabled(RedisConfig.scala:227)\n\tat com.redislabs.provider.redis.RedisConfig.getNodes(RedisConfig.scala:367)\n\tat com.redislabs.provider.redis.RedisConfig.getHosts(RedisConfig.scala:267)\n\tat com.redislabs.provider.redis.RedisConfig.<init>(RedisConfig.scala:166)\n\tat com.redislabs.provider.redis.RedisConfig$.fromSparkConfAndParameters(RedisConfig.scala:154)\n\tat org.apache.spark.sql.redis.RedisSourceRelation.<init>(RedisSourceRelation.scala:34)\n\tat org.apache.spark.sql.redis.DefaultSource.createRelation(DefaultSource.scala:21)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: redis.clients.jedis.exceptions.JedisConnectionException: Failed to create socket.\n\tat redis.clients.jedis.DefaultJedisSocketFactory.createSocket(DefaultJedisSocketFactory.java:110)\n\tat redis.clients.jedis.Connection.connect(Connection.java:226)\n\tat redis.clients.jedis.BinaryClient.connect(BinaryClient.java:144)\n\tat redis.clients.jedis.BinaryJedis.connect(BinaryJedis.java:314)\n\tat redis.clients.jedis.BinaryJedis.initializeFromClientConfig(BinaryJedis.java:92)\n\tat redis.clients.jedis.BinaryJedis.<init>(BinaryJedis.java:297)\n\tat redis.clients.jedis.Jedis.<init>(Jedis.java:169)\n\tat redis.clients.jedis.JedisFactory.makeObject(JedisFactory.java:177)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:571)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:298)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:223)\n\tat redis.clients.jedis.util.Pool.getResource(Pool.java:75)\n\t... 42 more\nCaused by: java.net.ConnectException: Connection refused (Connection refused)\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:607)\n\tat redis.clients.jedis.DefaultJedisSocketFactory.createSocket(DefaultJedisSocketFactory.java:80)\n\t... 53 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 레디스 연결\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# rd = redis.StrictRedis(host=REDIS_INFO[\"host\"], port=REDIS_INFO[\"port\"], db=0)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_df\u001b[38;5;241m.\u001b[39mcount() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# json_df = result_df.toJSON().collect()[0]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# rd.set(\"test_dfm_sample_eapp_data\", json_df)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     result_df\u001b[38;5;241m.\u001b[39mwrite \\\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.sql.redis\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_dfm_sample_eapp_data\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey.column\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_id\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:1107\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1107\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o118.save.\n: redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool\n\tat redis.clients.jedis.util.Pool.getResource(Pool.java:84)\n\tat redis.clients.jedis.JedisPool.getResource(JedisPool.java:377)\n\tat com.redislabs.provider.redis.ConnectionPool$.connect(ConnectionPool.scala:35)\n\tat com.redislabs.provider.redis.RedisEndpoint.connect(RedisConfig.scala:94)\n\tat com.redislabs.provider.redis.RedisConfig.clusterEnabled(RedisConfig.scala:227)\n\tat com.redislabs.provider.redis.RedisConfig.getNodes(RedisConfig.scala:367)\n\tat com.redislabs.provider.redis.RedisConfig.getHosts(RedisConfig.scala:267)\n\tat com.redislabs.provider.redis.RedisConfig.<init>(RedisConfig.scala:166)\n\tat com.redislabs.provider.redis.RedisConfig$.fromSparkConfAndParameters(RedisConfig.scala:154)\n\tat org.apache.spark.sql.redis.RedisSourceRelation.<init>(RedisSourceRelation.scala:34)\n\tat org.apache.spark.sql.redis.DefaultSource.createRelation(DefaultSource.scala:21)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: redis.clients.jedis.exceptions.JedisConnectionException: Failed to create socket.\n\tat redis.clients.jedis.DefaultJedisSocketFactory.createSocket(DefaultJedisSocketFactory.java:110)\n\tat redis.clients.jedis.Connection.connect(Connection.java:226)\n\tat redis.clients.jedis.BinaryClient.connect(BinaryClient.java:144)\n\tat redis.clients.jedis.BinaryJedis.connect(BinaryJedis.java:314)\n\tat redis.clients.jedis.BinaryJedis.initializeFromClientConfig(BinaryJedis.java:92)\n\tat redis.clients.jedis.BinaryJedis.<init>(BinaryJedis.java:297)\n\tat redis.clients.jedis.Jedis.<init>(Jedis.java:169)\n\tat redis.clients.jedis.JedisFactory.makeObject(JedisFactory.java:177)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:571)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:298)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:223)\n\tat redis.clients.jedis.util.Pool.getResource(Pool.java:75)\n\t... 42 more\nCaused by: java.net.ConnectException: Connection refused (Connection refused)\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:607)\n\tat redis.clients.jedis.DefaultJedisSocketFactory.createSocket(DefaultJedisSocketFactory.java:80)\n\t... 53 more\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import json\n",
    "\n",
    "# 레디스 연결\n",
    "# rd = redis.StrictRedis(host=REDIS_INFO[\"host\"], port=REDIS_INFO[\"port\"], db=0)\n",
    "\n",
    "if result_df.count() > 0:\n",
    "    # json_df = result_df.toJSON().collect()[0]\n",
    "    # rd.set(\"test_dfm_sample_eapp_data\", json_df)\n",
    "    \n",
    "    result_df.write \\\n",
    "    .format(\"org.apache.spark.sql.redis\") \\\n",
    "    .option(\"table\", \"test_dfm_sample_eapp_data\") \\\n",
    "    .option(\"key.column\", \"review_id\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c36268e0-9555-457c-b241-b0ff2984fd0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-06T05:20:01.591964Z",
     "iopub.status.busy": "2022-10-06T05:20:01.590878Z",
     "iopub.status.idle": "2022-10-06T05:20:01.599470Z",
     "shell.execute_reply": "2022-10-06T05:20:01.598538Z",
     "shell.execute_reply.started": "2022-10-06T05:20:01.591883Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review_id': '2208300923271031', 'cust_id': '03c4f22b4d5b998916c8ae5358c68f9d1b89ff96a93bc4d723518ba7bcf3f0dd', 'gender_cd': '1', 'gender_nm': '남', 'age': '36', 'agrde_cd_10_unit': '30', 'store_cd': '1000', 'store_nm': '이마트 창동점', 'score': 10, 'comments': '저렴하고 맛있어요!!!!!!!!!', 'comments_length': 18, 'sku_cd': '8809676682501', 'sku_nm': '(SF)피코크 된장찌개 요리재료', 'order_dt': '2022-08-29', 'dt': '2022-08-30', 'prdt_cat_cd': '13', 'prdt_cat_nm': '채소', 'prdt_di_cd': '10', 'prdt_di_nm': '신선1담당', 'prdt_gcode_cd': '130', 'prdt_gcode_nm': '간편채소', 'prdt_mcode_cd': '0438', 'prdt_mcode_nm': 'PEACOCK밀키트', 'prdt_dcode_cd': '0263', 'prdt_dcode_nm': 'P)한식(국탕)밀키트', 'area_cd': '00', 'area_nm': '서울', 'longitude': 127.046690683, 'latitude': 37.651608368, 'comments_point': 10, 'image_point': 10, 'thumb_point': 0, 'action_cd': '003', 'blind_flag': 'N', 'active_flag': 'Y', 'tag_list': '130::1::10,130::2::10,130::3::10,130::4::10,130::5::10', 'avg_tag_score': 10.0}\n"
     ]
    }
   ],
   "source": [
    "result = rd.get(\"test_dfm_sample_eapp_data\")\n",
    "result = result.decode('utf-8')\n",
    "result = dict(json.loads(result))\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4dc5439-8f16-4be0-b910-7ba09367e1aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-11T07:36:39.598217Z",
     "iopub.status.busy": "2022-10-11T07:36:39.597494Z",
     "iopub.status.idle": "2022-10-11T07:36:40.470057Z",
     "shell.execute_reply": "2022-10-11T07:36:40.469469Z",
     "shell.execute_reply.started": "2022-10-11T07:36:39.598185Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
