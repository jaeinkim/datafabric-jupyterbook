{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ff651f27-35d9-4ab7-94fa-8ab2cc59bdcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T08:02:12.550653Z",
     "iopub.status.busy": "2022-10-05T08:02:12.550099Z",
     "iopub.status.idle": "2022-10-05T08:02:12.555750Z",
     "shell.execute_reply": "2022-10-05T08:02:12.555099Z",
     "shell.execute_reply.started": "2022-10-05T08:02:12.550606Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT = \"emart-datafabric\"\n",
    "DATASET = \"common_dev\"\n",
    "TABLE = \"dfm_sample_eapp_data\"\n",
    "DT = \"2022-08-30\"\n",
    "LIMIT = 30\n",
    "\n",
    "REDIS_INFO = {\n",
    "    \"host\": \"redis-master.redis-farm.svc.cluster.local\",\n",
    "    \"port\": \"6379\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3cfd45ed-f648-4010-bbec-acc840039717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T07:00:55.623469Z",
     "iopub.status.busy": "2022-10-05T07:00:55.622981Z",
     "iopub.status.idle": "2022-10-05T07:01:03.172895Z",
     "shell.execute_reply": "2022-10-05T07:01:03.172266Z",
     "shell.execute_reply.started": "2022-10-05T07:00:55.623430Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/05 16:00:55 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "22/10/05 16:00:55 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "22/10/05 16:00:55 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/10/05 16:00:55 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "from pydatafabric.gcp import get_spark, bq_to_df\n",
    "spark = get_spark(extra_jars=\"gs://emart-datafabric-resources/jars/spark-redis_2.12-2.4.2.jar\")\n",
    "\n",
    "df = bq_to_df(f\"\"\"\n",
    "    select  *\n",
    "    from  `{PROJECT}.{DATASET}.{TABLE}`\n",
    "    WHERE dt = '{DT}'\n",
    "    limit {LIMIT}\n",
    "\"\"\", spark_session=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd5e0a3e-c44f-4636-8cc0-ed4706f8a3a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T07:01:04.563217Z",
     "iopub.status.busy": "2022-10-05T07:01:04.562838Z",
     "iopub.status.idle": "2022-10-05T07:01:04.567399Z",
     "shell.execute_reply": "2022-10-05T07:01:04.566628Z",
     "shell.execute_reply.started": "2022-10-05T07:01:04.563193Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.redis.host\", REDIS_INFO[\"host\"])\n",
    "spark.conf.set(\"spark.redis.port\", REDIS_INFO[\"port\"])\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "586b4954-3ae4-4b36-bb14-5adfeaa115f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T07:01:05.286957Z",
     "iopub.status.busy": "2022-10-05T07:01:05.286567Z",
     "iopub.status.idle": "2022-10-05T07:01:05.291772Z",
     "shell.execute_reply": "2022-10-05T07:01:05.291057Z",
     "shell.execute_reply.started": "2022-10-05T07:01:05.286930Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'redis-master.redis-farm.svc.cluster.local'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.redis.host\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7b6b116-f6b3-4d70-84c5-571cd08ca43b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T07:01:05.929205Z",
     "iopub.status.busy": "2022-10-05T07:01:05.928534Z",
     "iopub.status.idle": "2022-10-05T07:01:09.007365Z",
     "shell.execute_reply": "2022-10-05T07:01:09.006809Z",
     "shell.execute_reply.started": "2022-10-05T07:01:05.929179Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+---------+---------+---+----------------+--------+-------------+-----+---------------------------------+---------------+-------------+--------------------------------+----------+----------+-----------+-----------+----------+----------+-------------+-------------+-------------+------------------+-------------+------------------+-------+-------+-------------+------------+--------------+-----------+-----------+---------+----------+-----------+--------------------+-------------+\n",
      "|       review_id|             cust_id|gender_cd|gender_nm|age|agrde_cd_10_unit|store_cd|     store_nm|score|                         comments|comments_length|       sku_cd|                          sku_nm|  order_dt|        dt|prdt_cat_cd|prdt_cat_nm|prdt_di_cd|prdt_di_nm|prdt_gcode_cd|prdt_gcode_nm|prdt_mcode_cd|     prdt_mcode_nm|prdt_dcode_cd|     prdt_dcode_nm|area_cd|area_nm|    longitude|    latitude|comments_point|image_point|thumb_point|action_cd|blind_flag|active_flag|            tag_list|avg_tag_score|\n",
      "+----------------+--------------------+---------+---------+---+----------------+--------+-------------+-----+---------------------------------+---------------+-------------+--------------------------------+----------+----------+-----------+-----------+----------+----------+-------------+-------------+-------------+------------------+-------------+------------------+-------+-------+-------------+------------+--------------+-----------+-----------+---------+----------+-----------+--------------------+-------------+\n",
      "|2208300923271031|03c4f22b4d5b99891...|        1|       남| 36|              30|    1000|이마트 창동점|   10|       저렴하고 맛있어요!!!!!!!!!|             18|8809676682501|    (SF)피코크 된장찌개 요리재료|2022-08-29|2022-08-30|         13|       채소|        10| 신선1담당|          130|     간편채소|         0438|     PEACOCK밀키트|         0263|P)한식(국탕)밀키트|     00|   서울|127.046690683|37.651608368|            10|         10|          0|      003|         N|          Y|130::1::10,130::2...| 10.000000000|\n",
      "|2208300928379733|03c4f22b4d5b99891...|        1|       남| 36|              30|    1000|이마트 창동점|   10|                 싸서 좋아요!!!!!|             11|8801045522265|   오뚜기 진라면 매운맛 120g*5개|2022-08-29|2022-08-30|         34|      가공C|        30|  가공담당|          320|         면류|         0235|    봉지라면(국물)|         1485|        홍탕(봉지)|     00|   서울|127.046690683|37.651608368|            10|         10|          0|      003|         N|          Y|320::1::10,320::2...| 10.000000000|\n",
      "|2208300924173039|03c4f22b4d5b99891...|        1|       남| 36|              30|    1000|이마트 창동점|   10|           저렴하고 좋아요!!!!!!!|             15|8809489618278|  노브랜드 우리 쌀밥 한공기 210g|2022-08-29|2022-08-30|         34|      가공C|        30|  가공담당|          322|   상온간편식|         0073|노브랜드상온간편식|         0280|          N)즉석밥|     00|   서울|127.046690683|37.651608368|            10|         10|          0|      003|         N|          Y|322::1::10,322::2...| 10.000000000|\n",
      "|2208300925053511|03c4f22b4d5b99891...|        1|       남| 36|              30|    1000|이마트 창동점|   10|                맛있어요!!!!!!!!!|             13|8801043005821|      농심포테토칩오리지널(125g)|2022-08-29|2022-08-30|         34|      가공C|        30|  가공담당|          340|         과자|         0150|              스낵|         0278|    감자고구마스낵|     00|   서울|127.046690683|37.651608368|            10|         10|          0|      003|         N|          Y|340::1::10,340::2...| 10.000000000|\n",
      "|2208300929267098|03c4f22b4d5b99891...|        1|       남| 36|              30|    1000|이마트 창동점|   10|                 맛있어요!!!!!!!!|             12|8801117747602|            오리온도도한나쵸155G|2022-08-29|2022-08-30|         34|      가공C|        30|  가공담당|          340|         과자|         0150|              스낵|         1118|          나쵸팝콘|     00|   서울|127.046690683|37.651608368|            10|         10|          0|      003|         N|          Y|340::1::10,340::2...| 10.000000000|\n",
      "|2208300930213869|03c4f22b4d5b99891...|        1|       남| 36|              30|    1000|이마트 창동점|   10|               맛있는데 비싸요!!!|             11|8801111949934|크라운 쿠크다스 폴바셋 커피 289g|2022-08-29|2022-08-30|         34|      가공C|        30|  가공담당|          340|         과자|         0950|            비스킷|         1126|          일반쿠키|     00|   서울|127.046690683|37.651608368|            10|         10|          0|      003|         N|          Y|340::1::10,340::2...|  9.000000000|\n",
      "|2208301136446921|28d1c69d1a2a3cc6b...|        2|       여| 54|              50|    1000|이마트 창동점|   10|맛과 신선도 용량은 만족하지만 ...|             29|8809069301064|       Dole 스위티오 바나나 10CP|2022-08-29|2022-08-30|         11|       과일|        10| 신선1담당|          111|     수입과일|         9511|            바나나|         5835|    바나나(고산지)|     00|   서울|127.046690683|37.651608368|            10|         10|          0|      003|         N|          Y|111::1::10,111::2...|  9.000000000|\n",
      "|2208301122086191|28d1c69d1a2a3cc6b...|        2|       여| 54|              50|    1000|이마트 창동점|   10|가격 할인은 아니지만 신선도와 ...|             27|2429230000000|          (대)국내산삼겹살구이용|2022-08-29|2022-08-30|         14|       축산|        11| 신선2담당|          141|         돈육|         9571|          일반돈육|         5299|            삼겹살|     00|   서울|127.046690683|37.651608368|            10|         10|          0|      003|         N|          Y|141::1::10,141::2...| 10.000000000|\n",
      "|2208301122560242|28d1c69d1a2a3cc6b...|        2|       여| 54|              50|    1000|이마트 창동점|   10|가격 할인과 함께 신선도와 용량...|             27|1305650000000|   무항생제더느림+돈앞다리수육용|2022-08-29|2022-08-30|         14|       축산|        11| 신선2담당|          141|         돈육|         1580|        브랜드돈육|         5293|    브랜드돈앞다리|     00|   서울|127.046690683|37.651608368|            10|         10|          0|      003|         N|          Y|141::1::10,141::2...| 10.000000000|\n",
      "|2208301130070273|28d1c69d1a2a3cc6b...|        2|       여| 54|              50|    1000|이마트 창동점|   10|   1+1 행사 상품이라 구매하기 ...|             22|8801104280037|빙그레 요플레 클래식 플레인 8...|2022-08-29|2022-08-30|         33|      가공B|        30|  가공담당|          332|     유가공품|         0095|    떠먹는요구르트|         0633|    플레인요구르트|     00|   서울|127.046690683|37.651608368|            10|         10|          0|      003|         N|          Y|332::1::10,332::2...| 10.000000000|\n",
      "+----------------+--------------------+---------+---------+---+----------------+--------+-------------+-----+---------------------------------+---------------+-------------+--------------------------------+----------+----------+-----------+-----------+----------+----------+-------------+-------------+-------------+------------------+-------------+------------------+-------+-------+-------------+------------+--------------+-----------+-----------+---------+----------+-----------+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd9c3d22-3a76-4d13-b6ca-d95fb1f85f0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T07:01:12.545834Z",
     "iopub.status.busy": "2022-10-05T07:01:12.545458Z",
     "iopub.status.idle": "2022-10-05T07:01:12.554297Z",
     "shell.execute_reply": "2022-10-05T07:01:12.553772Z",
     "shell.execute_reply.started": "2022-10-05T07:01:12.545811Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(f\"temp_{TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "162c3e63-5877-45a1-946a-825114beea73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T07:01:13.269632Z",
     "iopub.status.busy": "2022-10-05T07:01:13.269214Z",
     "iopub.status.idle": "2022-10-05T07:01:16.758919Z",
     "shell.execute_reply": "2022-10-05T07:01:16.758271Z",
     "shell.execute_reply.started": "2022-10-05T07:01:13.269608Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+---------+---------+---+----------------+--------+-------------+-----+--------------------------+---------------+-------------+----------------------------+----------+----------+-----------+-----------+----------+----------+-------------+-------------+-------------+-------------+-------------+------------------+-------+-------+-------------+------------+--------------+-----------+-----------+---------+----------+-----------+--------------------+-------------+\n",
      "|       review_id|             cust_id|gender_cd|gender_nm|age|agrde_cd_10_unit|store_cd|     store_nm|score|                  comments|comments_length|       sku_cd|                      sku_nm|  order_dt|        dt|prdt_cat_cd|prdt_cat_nm|prdt_di_cd|prdt_di_nm|prdt_gcode_cd|prdt_gcode_nm|prdt_mcode_cd|prdt_mcode_nm|prdt_dcode_cd|     prdt_dcode_nm|area_cd|area_nm|    longitude|    latitude|comments_point|image_point|thumb_point|action_cd|blind_flag|active_flag|            tag_list|avg_tag_score|\n",
      "+----------------+--------------------+---------+---------+---+----------------+--------+-------------+-----+--------------------------+---------------+-------------+----------------------------+----------+----------+-----------+-----------+----------+----------+-------------+-------------+-------------+-------------+-------------+------------------+-------+-------+-------------+------------+--------------+-----------+-----------+---------+----------+-----------+--------------------+-------------+\n",
      "|2208300923271031|03c4f22b4d5b99891...|        1|       남| 36|              30|    1000|이마트 창동점|   10|저렴하고 맛있어요!!!!!!!!!|             18|8809676682501|(SF)피코크 된장찌개 요리재료|2022-08-29|2022-08-30|         13|       채소|        10| 신선1담당|          130|     간편채소|         0438|PEACOCK밀키트|         0263|P)한식(국탕)밀키트|     00|   서울|127.046690683|37.651608368|            10|         10|          0|      003|         N|          Y|130::1::10,130::2...| 10.000000000|\n",
      "+----------------+--------------------+---------+---------+---+----------------+--------+-------------+-----+--------------------------+---------------+-------------+----------------------------+----------+----------+-----------+-----------+----------+----------+-------------+-------------+-------------+-------------+-------------+------------------+-------+-------+-------------+------------+--------------+-----------+-----------+---------+----------+-----------+--------------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql(\n",
    "    f'select * from temp_{TABLE} limit 1')\n",
    "\n",
    "result_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83fc614c-2ce0-44e8-978c-0e1c837f5e8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T07:01:19.821903Z",
     "iopub.status.busy": "2022-10-05T07:01:19.821513Z",
     "iopub.status.idle": "2022-10-05T07:01:19.825731Z",
     "shell.execute_reply": "2022-10-05T07:01:19.825100Z",
     "shell.execute_reply.started": "2022-10-05T07:01:19.821870Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- cust_id: string (nullable = true)\n",
      " |-- gender_cd: string (nullable = true)\n",
      " |-- gender_nm: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- agrde_cd_10_unit: string (nullable = true)\n",
      " |-- store_cd: string (nullable = true)\n",
      " |-- store_nm: string (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      " |-- comments_length: long (nullable = true)\n",
      " |-- sku_cd: string (nullable = true)\n",
      " |-- sku_nm: string (nullable = true)\n",
      " |-- order_dt: date (nullable = true)\n",
      " |-- dt: date (nullable = true)\n",
      " |-- prdt_cat_cd: string (nullable = true)\n",
      " |-- prdt_cat_nm: string (nullable = true)\n",
      " |-- prdt_di_cd: string (nullable = true)\n",
      " |-- prdt_di_nm: string (nullable = true)\n",
      " |-- prdt_gcode_cd: string (nullable = true)\n",
      " |-- prdt_gcode_nm: string (nullable = true)\n",
      " |-- prdt_mcode_cd: string (nullable = true)\n",
      " |-- prdt_mcode_nm: string (nullable = true)\n",
      " |-- prdt_dcode_cd: string (nullable = true)\n",
      " |-- prdt_dcode_nm: string (nullable = true)\n",
      " |-- area_cd: string (nullable = true)\n",
      " |-- area_nm: string (nullable = true)\n",
      " |-- longitude: decimal(38,9) (nullable = true)\n",
      " |-- latitude: decimal(38,9) (nullable = true)\n",
      " |-- comments_point: long (nullable = true)\n",
      " |-- image_point: long (nullable = true)\n",
      " |-- thumb_point: long (nullable = true)\n",
      " |-- action_cd: string (nullable = true)\n",
      " |-- blind_flag: string (nullable = true)\n",
      " |-- active_flag: string (nullable = true)\n",
      " |-- tag_list: string (nullable = true)\n",
      " |-- avg_tag_score: decimal(38,9) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6bece4a1-6425-4e3c-a28e-acbfb51ace59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T07:01:26.688824Z",
     "iopub.status.busy": "2022-10-05T07:01:26.688447Z",
     "iopub.status.idle": "2022-10-05T07:01:27.100694Z",
     "shell.execute_reply": "2022-10-05T07:01:27.099942Z",
     "shell.execute_reply.started": "2022-10-05T07:01:26.688803Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o732.save.\n: java.lang.NoClassDefFoundError: redis/clients/jedis/PipelineBase\n\tat org.apache.spark.sql.redis.DefaultSource.createRelation(DefaultSource.scala:21)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: redis.clients.jedis.PipelineBase\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 33 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_df\u001b[38;5;241m.\u001b[39mcount() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     result_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.sql.redis\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_dfm_sample_eapp_data\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey.column\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_id\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:1107\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1107\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o732.save.\n: java.lang.NoClassDefFoundError: redis/clients/jedis/PipelineBase\n\tat org.apache.spark.sql.redis.DefaultSource.createRelation(DefaultSource.scala:21)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: redis.clients.jedis.PipelineBase\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 33 more\n"
     ]
    }
   ],
   "source": [
    "if result_df.count() > 0:\n",
    "    result_df.write.format(\"org.apache.spark.sql.redis\") \\\n",
    "    .option(\"table\", \"test_dfm_sample_eapp_data\") \\\n",
    "    .option(\"key.column\", \"review_id\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4dc5439-8f16-4be0-b910-7ba09367e1aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T07:09:18.248776Z",
     "iopub.status.busy": "2022-10-05T07:09:18.248336Z",
     "iopub.status.idle": "2022-10-05T07:09:18.926549Z",
     "shell.execute_reply": "2022-10-05T07:09:18.925957Z",
     "shell.execute_reply.started": "2022-10-05T07:09:18.248751Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27682515-53d8-4669-a818-6236d8d557ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T06:47:07.099789Z",
     "iopub.status.busy": "2022-10-05T06:47:07.099378Z",
     "iopub.status.idle": "2022-10-05T06:47:07.156555Z",
     "shell.execute_reply": "2022-10-05T06:47:07.156032Z",
     "shell.execute_reply.started": "2022-10-05T06:47:07.099766Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import redis\n",
    "import json\n",
    "\n",
    "# 레디스 연결\n",
    "rd = redis.StrictRedis(host=REDIS_INFO[\"host\"], port=REDIS_INFO[\"port\"], db=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "685326ea-bca5-426a-9dc8-51ff9953f92f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T06:47:23.489978Z",
     "iopub.status.busy": "2022-10-05T06:47:23.489598Z",
     "iopub.status.idle": "2022-10-05T06:47:23.493101Z",
     "shell.execute_reply": "2022-10-05T06:47:23.492584Z",
     "shell.execute_reply.started": "2022-10-05T06:47:23.489951Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dict 데이터 선언\n",
    "data = {\n",
    "    \"key1\": \"테스트값1\",\n",
    "    \"key2\": \"테스트값2\",\n",
    "    \"key3\": \"테스트값3\",\n",
    "    \"key4\": \"테스트값4\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4a85437-1fd1-4ddc-b698-6294e9ac514f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T06:47:25.101751Z",
     "iopub.status.busy": "2022-10-05T06:47:25.101336Z",
     "iopub.status.idle": "2022-10-05T06:47:25.107185Z",
     "shell.execute_reply": "2022-10-05T06:47:25.106605Z",
     "shell.execute_reply.started": "2022-10-05T06:47:25.101722Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "json_data_dict = json.dumps(data, ensure_ascii=False).encode('utf-8')\n",
    "\n",
    "# 데이터 set\n",
    "rd.set(\"dict\", json_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f031fe9b-9e08-476f-b266-1021146e877b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T06:47:26.078287Z",
     "iopub.status.busy": "2022-10-05T06:47:26.077560Z",
     "iopub.status.idle": "2022-10-05T06:47:26.081376Z",
     "shell.execute_reply": "2022-10-05T06:47:26.080884Z",
     "shell.execute_reply.started": "2022-10-05T06:47:26.078260Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 데이터 get\n",
    "result = rd.get(\"dict\")\n",
    "result = result.decode('utf-8')\n",
    "\n",
    "# json loads \n",
    "result = dict(json.loads(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3b0d3d3-da7e-4af3-ba7a-f4a515c05fde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T06:47:26.576963Z",
     "iopub.status.busy": "2022-10-05T06:47:26.576601Z",
     "iopub.status.idle": "2022-10-05T06:47:26.580058Z",
     "shell.execute_reply": "2022-10-05T06:47:26.579539Z",
     "shell.execute_reply.started": "2022-10-05T06:47:26.576942Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key1': '테스트값1', 'key2': '테스트값2', 'key3': '테스트값3', 'key4': '테스트값4'}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f491f228-11fe-409e-b3a6-7b6cb00087ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T07:13:59.163250Z",
     "iopub.status.busy": "2022-10-05T07:13:59.162785Z",
     "iopub.status.idle": "2022-10-05T07:13:59.166216Z",
     "shell.execute_reply": "2022-10-05T07:13:59.165719Z",
     "shell.execute_reply.started": "2022-10-05T07:13:59.163211Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f3afc21-72e2-4b70-b5d6-5c868f949948",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T07:13:59.663289Z",
     "iopub.status.busy": "2022-10-05T07:13:59.662924Z",
     "iopub.status.idle": "2022-10-05T07:13:59.668658Z",
     "shell.execute_reply": "2022-10-05T07:13:59.668206Z",
     "shell.execute_reply.started": "2022-10-05T07:13:59.663266Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f1dcb0ab760>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkConf = SparkConf()\n",
    "sparkConf.set(\"spark.jars\", \"gs://emart-datafabric-resources/jars/spark-redis_2.12-2.4.2.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "57e03a57-98f8-444a-92e7-5596082618f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T07:34:51.158399Z",
     "iopub.status.busy": "2022-10-05T07:34:51.157783Z",
     "iopub.status.idle": "2022-10-05T07:34:51.167088Z",
     "shell.execute_reply": "2022-10-05T07:34:51.166461Z",
     "shell.execute_reply.started": "2022-10-05T07:34:51.158352Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f1dcb0ab760>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkConf.set(\"spark.redis.host\", REDIS_INFO['host'])\n",
    "sparkConf.set(\"spark.redis.port\", REDIS_INFO['port'])\n",
    "sparkConf.set(\"spark.redis.auth\", \"\")\n",
    "sparkConf.set('spark.sql.execution.arrow.pyspark.enabled', 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0e0e053e-1799-457f-85ad-6ae1dbbe9a66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T08:00:14.336475Z",
     "iopub.status.busy": "2022-10-05T08:00:14.335885Z",
     "iopub.status.idle": "2022-10-05T08:00:14.359601Z",
     "shell.execute_reply": "2022-10-05T08:00:14.358896Z",
     "shell.execute_reply.started": "2022-10-05T08:00:14.336421Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.dynamicAllocation.minExecutors', '1'),\n",
       " ('spark.app.name', 'emart-x223726-959e04fb-642e-4bae-abb8-16ffb524b719'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:/tmp/spark-496a5e59-37d3-4fdc-85be-3b89f99b096a/spark-bigquery-with-dependencies_2.12-latest.jar,file:/tmp/spark-496a5e59-37d3-4fdc-85be-3b89f99b096a/spark-redis_2.12-3.1.0.jar'),\n",
       " ('spark.redis.port', '6379'),\n",
       " ('spark.yarn.am.memory', '640m'),\n",
       " ('spark.port.maxRetries', '128'),\n",
       " ('spark.history.fs.logDirectory',\n",
       "  'gs://dataproc-temp-asia-northeast3-149277781422-nozf8jsl/afff9a2a-167f-4877-a5f6-2dd534eea3ab/spark-job-history'),\n",
       " ('spark.redis.host', 'redis-master.redis-farm.svc.cluster.local'),\n",
       " ('spark.driver.memory', '6g'),\n",
       " ('spark.executor.instances', '2'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '200'),\n",
       " ('spark.yarn.unmanagedAM.enabled', 'true'),\n",
       " ('spark.driver.host', '198.19.39.8'),\n",
       " ('spark.executor.memory', '8g'),\n",
       " ('spark.yarn.historyServer.address', 'yellowelephant-m:18080'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.extraListeners',\n",
       "  'com.google.cloud.spark.performance.DataprocMetricsListener'),\n",
       " ('spark.executor.cores', '8'),\n",
       " ('spark.sql.execution.arrow.pyspark.enabled', 'true'),\n",
       " ('spark.dynamicAllocation.shuffleTracking.enabled', 'true'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1643981893724_112121'),\n",
       " ('spark.sql.cbo.joinReorder.enabled', 'true'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.metrics.namespace',\n",
       "  'app_name:${spark.app.name}.app_id:${spark.app.id}'),\n",
       " ('spark.scheduler.mode', 'FAIR'),\n",
       " ('spark.sql.adaptive.enabled', 'true'),\n",
       " ('spark.yarn.jars', 'local:/usr/lib/spark/jars/*'),\n",
       " ('spark.yarn.queue', 'default'),\n",
       " ('spark.scheduler.minRegisteredResourcesRatio', '0.0'),\n",
       " ('spark.hadoop.hive.execution.engine', 'mr'),\n",
       " ('spark.jars',\n",
       "  'gs://emart-datafabric-resources/jars/spark-redis_2.12-2.4.2.jar'),\n",
       " ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2'),\n",
       " ('spark.rpc.message.maxSize', '1024'),\n",
       " ('spark.redis.auth', ''),\n",
       " ('spark.driver.maxResultSize', '6g'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.ui.port', '0'),\n",
       " ('spark.sql.autoBroadcastJoinThreshold', '200m'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.eventLog.dir',\n",
       "  'gs://dataproc-temp-asia-northeast3-149277781422-nozf8jsl/afff9a2a-167f-4877-a5f6-2dd534eea3ab/spark-job-history'),\n",
       " ('spark.ui.enabled', 'false'),\n",
       " ('spark.executorEnv.OPENBLAS_NUM_THREADS', '1'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.yarn.dist.jars',\n",
       "  'hdfs://yellowelephant-m/jars/spark-bigquery-with-dependencies_2.12-latest.jar,gs://emart-datafabric-resources/jars/spark-redis_2.12-3.1.0.jar'),\n",
       " ('spark.sql.cbo.enabled', 'true')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkConf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "803f680c-2a66-4b6b-a58c-2cdbb05e054c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T08:01:26.935715Z",
     "iopub.status.busy": "2022-10-05T08:01:26.935176Z",
     "iopub.status.idle": "2022-10-05T08:01:26.966566Z",
     "shell.execute_reply": "2022-10-05T08:01:26.965816Z",
     "shell.execute_reply.started": "2022-10-05T08:01:26.935671Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=sparkConf).enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1ea386d1-d15c-4639-b86e-6907d51e5f73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T08:01:35.866601Z",
     "iopub.status.busy": "2022-10-05T08:01:35.866066Z",
     "iopub.status.idle": "2022-10-05T08:01:35.878340Z",
     "shell.execute_reply": "2022-10-05T08:01:35.877601Z",
     "shell.execute_reply.started": "2022-10-05T08:01:35.866558Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\" select 1 as num \"\"\").registerTempTable('test_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0ea6fddb-deef-45b0-baaf-47fe0922aaf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T08:01:36.988395Z",
     "iopub.status.busy": "2022-10-05T08:01:36.987876Z",
     "iopub.status.idle": "2022-10-05T08:01:36.994698Z",
     "shell.execute_reply": "2022-10-05T08:01:36.994025Z",
     "shell.execute_reply.started": "2022-10-05T08:01:36.988345Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "    select\n",
    "        *\n",
    "    from \n",
    "        test_table\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5f223246-02c1-473a-82b8-56bac78b7415",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T08:01:52.982358Z",
     "iopub.status.busy": "2022-10-05T08:01:52.981912Z",
     "iopub.status.idle": "2022-10-05T08:01:53.861071Z",
     "shell.execute_reply": "2022-10-05T08:01:53.860353Z",
     "shell.execute_reply.started": "2022-10-05T08:01:52.982316Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ac3f0602-98ee-4e74-9b3a-bf9c99775272",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T08:01:58.454472Z",
     "iopub.status.busy": "2022-10-05T08:01:58.453948Z",
     "iopub.status.idle": "2022-10-05T08:01:58.538176Z",
     "shell.execute_reply": "2022-10-05T08:01:58.537159Z",
     "shell.execute_reply.started": "2022-10-05T08:01:58.454428Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2542.save.\n: java.lang.NoClassDefFoundError: redis/clients/jedis/PipelineBase\n\tat org.apache.spark.sql.redis.DefaultSource.createRelation(DefaultSource.scala:21)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat sun.reflect.GeneratedMethodAccessor145.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: redis.clients.jedis.PipelineBase\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 32 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [108], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.sql.redis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_table_test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey.column\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msave(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:1107\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1107\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2542.save.\n: java.lang.NoClassDefFoundError: redis/clients/jedis/PipelineBase\n\tat org.apache.spark.sql.redis.DefaultSource.createRelation(DefaultSource.scala:21)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat sun.reflect.GeneratedMethodAccessor145.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: redis.clients.jedis.PipelineBase\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 32 more\n"
     ]
    }
   ],
   "source": [
    "df.write.format(\"org.apache.spark.sql.redis\").option(\"table\", \"test_table_test\").option(\"key.column\", \"num\").save(mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b25dde62-c471-482b-a77a-0f56b72ea16b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T08:02:01.320896Z",
     "iopub.status.busy": "2022-10-05T08:02:01.320371Z",
     "iopub.status.idle": "2022-10-05T08:02:02.045934Z",
     "shell.execute_reply": "2022-10-05T08:02:02.045167Z",
     "shell.execute_reply.started": "2022-10-05T08:02:01.320850Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea65f5e-a2ef-4a9d-98a7-7fea31896dec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
