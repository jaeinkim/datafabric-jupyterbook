{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff651f27-35d9-4ab7-94fa-8ab2cc59bdcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T06:17:48.114414Z",
     "iopub.status.busy": "2022-10-07T06:17:48.113992Z",
     "iopub.status.idle": "2022-10-07T06:17:48.118139Z",
     "shell.execute_reply": "2022-10-07T06:17:48.117602Z",
     "shell.execute_reply.started": "2022-10-07T06:17:48.114387Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT = \"emart-datafabric\"\n",
    "DATASET = \"common_dev\"\n",
    "# TABLE = \"dfm_sample_eapp_data\"\n",
    "TABLE = \"test_spark_to_redis\"\n",
    "DT = \"2022-08-30\"\n",
    "LIMIT = 30\n",
    "\n",
    "REDIS_INFO = {\n",
    "    \"host\": \"redis-master.redis-farm.svc.cluster.local\",\n",
    "    \"port\": \"6379\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cfd45ed-f648-4010-bbec-acc840039717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T06:13:27.378313Z",
     "iopub.status.busy": "2022-10-07T06:13:27.377903Z",
     "iopub.status.idle": "2022-10-07T06:13:27.418645Z",
     "shell.execute_reply": "2022-10-07T06:13:27.417994Z",
     "shell.execute_reply.started": "2022-10-07T06:13:27.378287Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydatafabric.gcp import get_spark, bq_to_df\n",
    "\n",
    "spark = get_spark(extra_jars=\"gs://emart-datafabric-resources/jars/spark-redis_2.12-3.1.0-SNAPSHOT-jar-with-dependencies.jar\")\n",
    "spark.conf.set(\"spark.redis.host\", REDIS_INFO[\"host\"])\n",
    "spark.conf.set(\"spark.redis.port\", REDIS_INFO[\"port\"])\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "df = bq_to_df(f\"\"\"\n",
    "    select  *\n",
    "    from  `{PROJECT}.{DATASET}.{TABLE}`\n",
    "    WHERE dt = '{DT}'\n",
    "    limit {LIMIT}\n",
    "\"\"\", spark_session=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30a6a08e-fe3f-4d03-bf45-c673897f89f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T06:13:28.883746Z",
     "iopub.status.busy": "2022-10-07T06:13:28.883352Z",
     "iopub.status.idle": "2022-10-07T06:13:28.921879Z",
     "shell.execute_reply": "2022-10-07T06:13:28.921335Z",
     "shell.execute_reply.started": "2022-10-07T06:13:28.883723Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "\n",
    "columns = ['id', 'dogs', 'cats']\n",
    "vals = [(1, 2, 0), (2, 0, 1)]\n",
    "\n",
    "df = spark.createDataFrame(vals, columns)\n",
    "\n",
    "newRow = spark.createDataFrame([(4,5,7)], columns)\n",
    "df = df.union(newRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54ce5b2b-53fd-44ba-9023-52ba8952512f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T06:13:30.585654Z",
     "iopub.status.busy": "2022-10-07T06:13:30.585257Z",
     "iopub.status.idle": "2022-10-07T06:13:30.590143Z",
     "shell.execute_reply": "2022-10-07T06:13:30.589560Z",
     "shell.execute_reply.started": "2022-10-07T06:13:30.585630Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hdfs:///jars/spark-bigquery-with-dependencies_2.12-latest.jar,gs://emart-datafabric-resources/jars/spark-redis_2.12-3.1.0-SNAPSHOT-jar-with-dependencies.jar'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.jars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7b6b116-f6b3-4d70-84c5-571cd08ca43b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T06:13:31.185388Z",
     "iopub.status.busy": "2022-10-07T06:13:31.184994Z",
     "iopub.status.idle": "2022-10-07T06:13:31.390922Z",
     "shell.execute_reply": "2022-10-07T06:13:31.390364Z",
     "shell.execute_reply.started": "2022-10-07T06:13:31.185362Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "| id|dogs|cats|\n",
      "+---+----+----+\n",
      "|  1|   2|   0|\n",
      "|  2|   0|   1|\n",
      "|  4|   5|   7|\n",
      "+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd9c3d22-3a76-4d13-b6ca-d95fb1f85f0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T06:13:33.846540Z",
     "iopub.status.busy": "2022-10-07T06:13:33.846024Z",
     "iopub.status.idle": "2022-10-07T06:13:33.856455Z",
     "shell.execute_reply": "2022-10-07T06:13:33.855727Z",
     "shell.execute_reply.started": "2022-10-07T06:13:33.846501Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(f\"temp_{TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "162c3e63-5877-45a1-946a-825114beea73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T06:13:34.328464Z",
     "iopub.status.busy": "2022-10-07T06:13:34.327964Z",
     "iopub.status.idle": "2022-10-07T06:13:34.571928Z",
     "shell.execute_reply": "2022-10-07T06:13:34.571195Z",
     "shell.execute_reply.started": "2022-10-07T06:13:34.328421Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "| id|dogs|cats|\n",
      "+---+----+----+\n",
      "|  1|   2|   0|\n",
      "+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql(\n",
    "    f'select * from temp_{TABLE} limit 1')\n",
    "\n",
    "result_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83fc614c-2ce0-44e8-978c-0e1c837f5e8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T06:13:35.548099Z",
     "iopub.status.busy": "2022-10-07T06:13:35.547569Z",
     "iopub.status.idle": "2022-10-07T06:13:35.552727Z",
     "shell.execute_reply": "2022-10-07T06:13:35.552045Z",
     "shell.execute_reply.started": "2022-10-07T06:13:35.548063Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- dogs: long (nullable = true)\n",
      " |-- cats: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bece4a1-6425-4e3c-a28e-acbfb51ace59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T06:55:48.079404Z",
     "iopub.status.busy": "2022-10-07T06:55:48.079008Z",
     "iopub.status.idle": "2022-10-07T06:55:48.301796Z",
     "shell.execute_reply": "2022-10-07T06:55:48.300988Z",
     "shell.execute_reply.started": "2022-10-07T06:55:48.079375Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o319.save.\n: redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool\n\tat redis.clients.jedis.util.Pool.getResource(Pool.java:84)\n\tat redis.clients.jedis.JedisPool.getResource(JedisPool.java:377)\n\tat com.redislabs.provider.redis.ConnectionPool$.connect(ConnectionPool.scala:35)\n\tat com.redislabs.provider.redis.RedisEndpoint.connect(RedisConfig.scala:94)\n\tat com.redislabs.provider.redis.RedisConfig.clusterEnabled(RedisConfig.scala:227)\n\tat com.redislabs.provider.redis.RedisConfig.getNodes(RedisConfig.scala:367)\n\tat com.redislabs.provider.redis.RedisConfig.getHosts(RedisConfig.scala:267)\n\tat com.redislabs.provider.redis.RedisConfig.<init>(RedisConfig.scala:166)\n\tat com.redislabs.provider.redis.RedisConfig$.fromSparkConfAndParameters(RedisConfig.scala:154)\n\tat org.apache.spark.sql.redis.RedisSourceRelation.<init>(RedisSourceRelation.scala:34)\n\tat org.apache.spark.sql.redis.DefaultSource.createRelation(DefaultSource.scala:21)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: redis.clients.jedis.exceptions.JedisConnectionException: Failed to create socket.\n\tat redis.clients.jedis.DefaultJedisSocketFactory.createSocket(DefaultJedisSocketFactory.java:110)\n\tat redis.clients.jedis.Connection.connect(Connection.java:226)\n\tat redis.clients.jedis.BinaryClient.connect(BinaryClient.java:144)\n\tat redis.clients.jedis.BinaryJedis.connect(BinaryJedis.java:314)\n\tat redis.clients.jedis.BinaryJedis.initializeFromClientConfig(BinaryJedis.java:92)\n\tat redis.clients.jedis.BinaryJedis.<init>(BinaryJedis.java:297)\n\tat redis.clients.jedis.Jedis.<init>(Jedis.java:169)\n\tat redis.clients.jedis.JedisFactory.makeObject(JedisFactory.java:177)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:571)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:298)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:223)\n\tat redis.clients.jedis.util.Pool.getResource(Pool.java:75)\n\t... 42 more\nCaused by: java.net.ConnectException: Connection refused (Connection refused)\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:607)\n\tat redis.clients.jedis.DefaultJedisSocketFactory.createSocket(DefaultJedisSocketFactory.java:80)\n\t... 53 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 레디스 연결\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# rd = redis.StrictRedis(host=REDIS_INFO[\"host\"], port=REDIS_INFO[\"port\"], db=0)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_df\u001b[38;5;241m.\u001b[39mcount() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# json_df = result_df.toJSON().collect()[0]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# rd.set(\"test_dfm_sample_eapp_data\", json_df)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     result_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.sql.redis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_dfm_sample_eapp_data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:1107\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1107\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o319.save.\n: redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool\n\tat redis.clients.jedis.util.Pool.getResource(Pool.java:84)\n\tat redis.clients.jedis.JedisPool.getResource(JedisPool.java:377)\n\tat com.redislabs.provider.redis.ConnectionPool$.connect(ConnectionPool.scala:35)\n\tat com.redislabs.provider.redis.RedisEndpoint.connect(RedisConfig.scala:94)\n\tat com.redislabs.provider.redis.RedisConfig.clusterEnabled(RedisConfig.scala:227)\n\tat com.redislabs.provider.redis.RedisConfig.getNodes(RedisConfig.scala:367)\n\tat com.redislabs.provider.redis.RedisConfig.getHosts(RedisConfig.scala:267)\n\tat com.redislabs.provider.redis.RedisConfig.<init>(RedisConfig.scala:166)\n\tat com.redislabs.provider.redis.RedisConfig$.fromSparkConfAndParameters(RedisConfig.scala:154)\n\tat org.apache.spark.sql.redis.RedisSourceRelation.<init>(RedisSourceRelation.scala:34)\n\tat org.apache.spark.sql.redis.DefaultSource.createRelation(DefaultSource.scala:21)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: redis.clients.jedis.exceptions.JedisConnectionException: Failed to create socket.\n\tat redis.clients.jedis.DefaultJedisSocketFactory.createSocket(DefaultJedisSocketFactory.java:110)\n\tat redis.clients.jedis.Connection.connect(Connection.java:226)\n\tat redis.clients.jedis.BinaryClient.connect(BinaryClient.java:144)\n\tat redis.clients.jedis.BinaryJedis.connect(BinaryJedis.java:314)\n\tat redis.clients.jedis.BinaryJedis.initializeFromClientConfig(BinaryJedis.java:92)\n\tat redis.clients.jedis.BinaryJedis.<init>(BinaryJedis.java:297)\n\tat redis.clients.jedis.Jedis.<init>(Jedis.java:169)\n\tat redis.clients.jedis.JedisFactory.makeObject(JedisFactory.java:177)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:571)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:298)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:223)\n\tat redis.clients.jedis.util.Pool.getResource(Pool.java:75)\n\t... 42 more\nCaused by: java.net.ConnectException: Connection refused (Connection refused)\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:607)\n\tat redis.clients.jedis.DefaultJedisSocketFactory.createSocket(DefaultJedisSocketFactory.java:80)\n\t... 53 more\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import json\n",
    "\n",
    "# 레디스 연결\n",
    "# rd = redis.StrictRedis(host=REDIS_INFO[\"host\"], port=REDIS_INFO[\"port\"], db=0)\n",
    "\n",
    "if result_df.count() > 0:\n",
    "    # json_df = result_df.toJSON().collect()[0]\n",
    "    # rd.set(\"test_dfm_sample_eapp_data\", json_df)\n",
    "    \n",
    "    result_df.write.format(\"org.apache.spark.sql.redis\").option(\"table\", \"test_dfm_sample_eapp_data\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c36268e0-9555-457c-b241-b0ff2984fd0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-06T05:20:01.591964Z",
     "iopub.status.busy": "2022-10-06T05:20:01.590878Z",
     "iopub.status.idle": "2022-10-06T05:20:01.599470Z",
     "shell.execute_reply": "2022-10-06T05:20:01.598538Z",
     "shell.execute_reply.started": "2022-10-06T05:20:01.591883Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review_id': '2208300923271031', 'cust_id': '03c4f22b4d5b998916c8ae5358c68f9d1b89ff96a93bc4d723518ba7bcf3f0dd', 'gender_cd': '1', 'gender_nm': '남', 'age': '36', 'agrde_cd_10_unit': '30', 'store_cd': '1000', 'store_nm': '이마트 창동점', 'score': 10, 'comments': '저렴하고 맛있어요!!!!!!!!!', 'comments_length': 18, 'sku_cd': '8809676682501', 'sku_nm': '(SF)피코크 된장찌개 요리재료', 'order_dt': '2022-08-29', 'dt': '2022-08-30', 'prdt_cat_cd': '13', 'prdt_cat_nm': '채소', 'prdt_di_cd': '10', 'prdt_di_nm': '신선1담당', 'prdt_gcode_cd': '130', 'prdt_gcode_nm': '간편채소', 'prdt_mcode_cd': '0438', 'prdt_mcode_nm': 'PEACOCK밀키트', 'prdt_dcode_cd': '0263', 'prdt_dcode_nm': 'P)한식(국탕)밀키트', 'area_cd': '00', 'area_nm': '서울', 'longitude': 127.046690683, 'latitude': 37.651608368, 'comments_point': 10, 'image_point': 10, 'thumb_point': 0, 'action_cd': '003', 'blind_flag': 'N', 'active_flag': 'Y', 'tag_list': '130::1::10,130::2::10,130::3::10,130::4::10,130::5::10', 'avg_tag_score': 10.0}\n"
     ]
    }
   ],
   "source": [
    "result = rd.get(\"test_dfm_sample_eapp_data\")\n",
    "result = result.decode('utf-8')\n",
    "result = dict(json.loads(result))\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4dc5439-8f16-4be0-b910-7ba09367e1aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T06:06:14.154081Z",
     "iopub.status.busy": "2022-10-07T06:06:14.153644Z",
     "iopub.status.idle": "2022-10-07T06:06:14.989095Z",
     "shell.execute_reply": "2022-10-07T06:06:14.988469Z",
     "shell.execute_reply.started": "2022-10-07T06:06:14.154054Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf132e67-ee92-4340-bf94-74149bd9b54c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
