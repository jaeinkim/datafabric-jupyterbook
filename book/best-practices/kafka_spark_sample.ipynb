{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b132e6-352b-49c1-b8a3-f286408f923b",
   "metadata": {},
   "source": [
    "### pyspark read kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "74ba4565-2910-4a46-8d1b-f62bdcafa8d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-11T07:43:34.234146Z",
     "iopub.status.busy": "2022-10-11T07:43:34.233760Z",
     "iopub.status.idle": "2022-10-11T07:43:34.237421Z",
     "shell.execute_reply": "2022-10-11T07:43:34.236898Z",
     "shell.execute_reply.started": "2022-10-11T07:43:34.234123Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT = \"emart-datafabric\"\n",
    "DATASET = \"common_dev\"\n",
    "TABLE = \"dfm_sample_eapp_data\"\n",
    "LIMIT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4c8f30f8-ebb6-48ac-887e-9ce17467cb85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-11T07:43:34.700207Z",
     "iopub.status.busy": "2022-10-11T07:43:34.699526Z",
     "iopub.status.idle": "2022-10-11T07:43:34.703077Z",
     "shell.execute_reply": "2022-10-11T07:43:34.702587Z",
     "shell.execute_reply.started": "2022-10-11T07:43:34.700181Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "BOOTSTRAP_SERVERS = ['datafabric-kafka-kafka-0.datafabric-kafka-kafka-brokers.kafka-farm.svc.cluster.local:9092', 'datafabric-kafka-kafka-1.datafabric-kafka-kafka-brokers.kafka-farm.svc.cluster.local:9092','datafabric-kafka-kafka-2.datafabric-kafka-kafka-brokers.kafka-farm.svc.cluster.local:9092']\n",
    "TOPICS = 'datafabric'\n",
    "CONSUMER_GROUP = 'test-datafabric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0a88909d-2205-4ca6-b7f9-caad53c1ead4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-11T07:43:35.175174Z",
     "iopub.status.busy": "2022-10-11T07:43:35.174788Z",
     "iopub.status.idle": "2022-10-11T07:43:35.215236Z",
     "shell.execute_reply": "2022-10-11T07:43:35.214669Z",
     "shell.execute_reply.started": "2022-10-11T07:43:35.175149Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydatafabric.ye import get_spark\n",
    "from pydatafabric.gcp import bq_to_df\n",
    "\n",
    "spark = get_spark(extra_jars=\"gs://emart-datafabric-resources/jars/spark-sql-kafka-0-10_2.12-3.1.2.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6ed2ba13-a286-44bc-b589-f946d03cb472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-11T07:43:37.006869Z",
     "iopub.status.busy": "2022-10-11T07:43:37.006477Z",
     "iopub.status.idle": "2022-10-11T07:43:37.011152Z",
     "shell.execute_reply": "2022-10-11T07:43:37.010628Z",
     "shell.execute_reply.started": "2022-10-11T07:43:37.006844Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hdfs:///jars/spark-bigquery-with-dependencies_2.12-latest.jar,gs://emart-datafabric-resources/jars/spark-sql-kafka-0-10_2.12-3.1.2.jar'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.jars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c276770c-fd15-4044-a913-ce20f716f375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-11T07:31:37.825025Z",
     "iopub.status.busy": "2022-10-11T07:31:37.824633Z",
     "iopub.status.idle": "2022-10-11T07:31:46.014431Z",
     "shell.execute_reply": "2022-10-11T07:31:46.013803Z",
     "shell.execute_reply.started": "2022-10-11T07:31:37.825001Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = bq_to_df(f\"\"\"\n",
    "    select review_id, comments\n",
    "    from  `{PROJECT}.{DATASET}.{TABLE}`\n",
    "    limit {LIMIT}\n",
    "\"\"\", spark_session=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8b125de5-8110-4453-b71b-f7efabdd6a40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-11T07:31:46.981001Z",
     "iopub.status.busy": "2022-10-11T07:31:46.980614Z",
     "iopub.status.idle": "2022-10-11T07:31:50.807642Z",
     "shell.execute_reply": "2022-10-11T07:31:50.807079Z",
     "shell.execute_reply.started": "2022-10-11T07:31:46.980976Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------------------+\n",
      "|       review_id|                        comments|\n",
      "+----------------+--------------------------------+\n",
      "|2208051914367759|싱싱은 한데\n",
      "아무 맛도 안나요...|\n",
      "+----------------+--------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4ac3935c-d7bc-444a-8566-861601403d75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-11T07:31:51.824304Z",
     "iopub.status.busy": "2022-10-11T07:31:51.823896Z",
     "iopub.status.idle": "2022-10-11T07:31:51.827758Z",
     "shell.execute_reply": "2022-10-11T07:31:51.827247Z",
     "shell.execute_reply.started": "2022-10-11T07:31:51.824275Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01595bf5-6405-463c-a433-c7ca1d58d6c8",
   "metadata": {},
   "source": [
    "### spark from kafaka topic read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7996d6b6-3acc-4f2c-a52a-75f0c992fbc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-11T07:03:16.483203Z",
     "iopub.status.busy": "2022-10-11T07:03:16.482921Z",
     "iopub.status.idle": "2022-10-11T07:03:16.485899Z",
     "shell.execute_reply": "2022-10-11T07:03:16.485335Z",
     "shell.execute_reply.started": "2022-10-11T07:03:16.483182Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e88eaf70-2d2b-45da-9f1e-1fc7a020814c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-11T07:03:12.778282Z",
     "iopub.status.busy": "2022-10-11T07:03:12.778021Z",
     "iopub.status.idle": "2022-10-11T07:03:12.780955Z",
     "shell.execute_reply": "2022-10-11T07:03:12.780438Z",
     "shell.execute_reply.started": "2022-10-11T07:03:12.778262Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from kafka import KafkaProducer\n",
    "# from json import dumps\n",
    "# import time\n",
    "\n",
    "# producer = KafkaProducer(acks=0,\n",
    "#                          compression_type='gzip',\n",
    "#                          bootstrap_servers=BOOTSTRAP_SERVERS,\n",
    "#                          value_serializer=lambda x: dumps(x).encode('utf-8')\n",
    "#                          )\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# for i in range(10000):\n",
    "#     data = {'str': 'result' + str(i)}\n",
    "#     producer.send('test', value=data)\n",
    "#     producer.flush()\n",
    "\n",
    "# print(\"elapsed :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ebd7a2d4-ea19-4965-a886-86b75101ec60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-11T07:03:12.935532Z",
     "iopub.status.busy": "2022-10-11T07:03:12.935215Z",
     "iopub.status.idle": "2022-10-11T07:03:12.938314Z",
     "shell.execute_reply": "2022-10-11T07:03:12.937774Z",
     "shell.execute_reply.started": "2022-10-11T07:03:12.935512Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# from kafka import KafkaConsumer\n",
    "\n",
    "# consumer = KafkaConsumer(\n",
    "#      TOPICS,\n",
    "#      bootstrap_servers=BOOTSTRAP_SERVERS,\n",
    "#      enable_auto_commit=True,\n",
    "#      group_id=\"test1-group\",\n",
    "#      value_deserializer=lambda x: loads(x.decode('utf-8')),\n",
    "#      consumer_timeout_ms=1000\n",
    "# )\n",
    "\n",
    "# # # consumer 리스트를 가져온다\n",
    "# # print('[begin] get consumer list')\n",
    "# for message in consumer:\n",
    "#     print(\"Topic: %s, Partition: %d, Offset: %d, Key: %s, Value: %s\" % (\n",
    "#         message.topic, message.partition, message.offset, message.key, message.value\n",
    "#     ))\n",
    "# # print('[end] get consumer list')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fdfa4fe8-9b32-41bc-acf6-3ddb6b526c65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-11T07:43:25.697159Z",
     "iopub.status.busy": "2022-10-11T07:43:25.696722Z",
     "iopub.status.idle": "2022-10-11T07:43:25.744978Z",
     "shell.execute_reply": "2022-10-11T07:43:25.744022Z",
     "shell.execute_reply.started": "2022-10-11T07:43:25.697130Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1625.load.\n: java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.sql.kafka010.KafkaSourceProvider$\n\tat org.apache.spark.sql.kafka010.KafkaSourceProvider.org$apache$spark$sql$kafka010$KafkaSourceProvider$$validateBatchOptions(KafkaSourceProvider.scala:336)\n\tat org.apache.spark.sql.kafka010.KafkaSourceProvider.createRelation(KafkaSourceProvider.scala:127)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:355)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:225)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [93], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread \\\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      3\u001b[0m   \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubscribe\u001b[39m\u001b[38;5;124m\"\u001b[39m, TOPICS) \\\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.bootstrap.servers\u001b[39m\u001b[38;5;124m\"\u001b[39m, BOOTSTRAP_SERVERS) \\\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailOnDataLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubscribe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      7\u001b[0m   \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincludeHeaders\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      8\u001b[0m   \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartingOffsets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatest\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      9\u001b[0m   \u001b[38;5;241m.\u001b[39mload()\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:210\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1625.load.\n: java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.sql.kafka010.KafkaSourceProvider$\n\tat org.apache.spark.sql.kafka010.KafkaSourceProvider.org$apache$spark$sql$kafka010$KafkaSourceProvider$$validateBatchOptions(KafkaSourceProvider.scala:336)\n\tat org.apache.spark.sql.kafka010.KafkaSourceProvider.createRelation(KafkaSourceProvider.scala:127)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:355)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:225)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"subscribe\", TOPICS) \\\n",
    "  .option(\"kafka.bootstrap.servers\", BOOTSTRAP_SERVERS) \\\n",
    "  .option(\"failOnDataLoss\", \"false\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\\n",
    "  .option(\"includeHeaders\", \"true\") \\\n",
    "  .option(\"startingOffsets\", \"latest\") \\\n",
    "  .load()\n",
    "\n",
    "# df.write \\\n",
    "#   .format(\"kafka\") \\\n",
    "#   .option(\"kafka.bootstrap.servers\", BOOTSTRAP_SERVERS) \\\n",
    "#   .option(\"topic\", TOPICS) \\\n",
    "#   .save()\n",
    "\n",
    "# if starting_offsets is not None:\n",
    "#     print(starting_offsets)\n",
    "#     df_reader = df_reader \\\n",
    "#         .option(\"startingOffsets\", starting_offsets)\\\n",
    "#         .option(\"auto.offset.reset\", \"earliest\") \\\n",
    "#         .option(\"failOnDataLoss\", \"false\")\n",
    "    \n",
    "# if CONSUMER_CONFIGS != '':\n",
    "#     config_tokens = CONSUMER_CONFIGS.split(',')\n",
    "#     print(config_tokens)\n",
    "    \n",
    "#     for token in config_tokens:\n",
    "#         key_value = token.split('=')\n",
    "#         print(key_value)\n",
    "        \n",
    "#         df_reader = df_reader.option(f\"kafka.{key_value[0]}\", key_value[1])\n",
    "        \n",
    "# df = df_reader.load()\n",
    "# df.printSchema()\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6a332190-45f3-4f23-b560-b88ed108bed7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-11T07:29:48.454811Z",
     "iopub.status.busy": "2022-10-11T07:29:48.454565Z",
     "iopub.status.idle": "2022-10-11T07:29:49.471122Z",
     "shell.execute_reply": "2022-10-11T07:29:49.470529Z",
     "shell.execute_reply.started": "2022-10-11T07:29:48.454789Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ead720-dc9c-4251-9a04-bbeba1c79615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
